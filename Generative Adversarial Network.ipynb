{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Generative Adversarial Network (GANs) is that you have two Networks, a Generator G and a Discriminator D. The Generator is trained to fool the discriminator, it wants to output data that look as close as possible to real, training data. The discriminator also sees real training data and predicts if the data it's receive is real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "#how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "#convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#get the training datasets\n",
    "train_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "\n",
    "#prepare data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f551c182b70>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAC9RJREFUeJzt3X+IVXUax/H3s7b+kWu5QzSK6Zoilkk7C6ZRQknMpoth0y8aaBEU7Q8HbAlB/KdiMYSs3SRZdHcthdYMqnWSWA013aVlaDL7ZesW0dbkoEWaP/oh4zz7x5yJaeZ7v/fOvefee+7184KYe5/OnPtc6sM593vPPMfcHREJ+0m1GxDJMgVEJEIBEYlQQEQiFBCRCAVEJEIBEYlQQEQiFBCRiItK+WUzmwc8CYwA/uLua/Nsr6/tJTPc3fJtY8VeamJmI4D/As1AF/AG0OruhyO/o4BIZhQSkFJOsWYBH7n7x+5+DngOWFjC/kQyp5SAjAc+G/C8K6n9iJktM7NOM+ss4bVEqqKUzyChw9OQUyh33wRsAp1iSe0p5QjSBUwY8PwK4Ghp7YhkSykBeQOYamZXmtlI4F6gPZ22RLKh6FMsd+8xszZgF33LvJvd/f3UOhPJgKKXeYt6MX0GkQwp9zKvSN1TQEQiFBCRCAVEJEIBEYlQQEQiFBCRCAVEJEIBEYlQQEQiFBCRCAVEJEIBEYlQQEQiFBCRCAVEJEIBEYlQQEQiFBCRiJJm80o6RowYEaxfeumlqey/ra0tWL/44ouH1KZNmxbcdvny5cH6unXrgvXW1tZg/bvvvgvW164Nj3V+5JFHgvVKKXV49SfAaeA80OPuM9NoSiQr0jiCzHX3L1PYj0jm6DOISESpAXFgt5m9aWbLQhtoeLXUslJPsW5096Nmdjnwqpn9x90PDNxAw6ullpUUEHc/mvw8bmYv0XfPkAPx36pNEydODNZHjhwZrN9www1DanPmzAluO2bMmGD9zjvvLLC79HR1dQXr69evD9ZbWlqC9dOnTwfrb7/9drC+f//+ArqrvKJPscxslJmN7n8M/Bp4L63GRLKglCNII/CSmfXv52/u/o9UuhLJiFKmu38M/DLFXkQyR8u8IhEKiEiE7g8ySFNTU7C+d+/eYD2t66Wqobe3d0ht8eLFwW3PnDkzrH13d3cH6ydOnAjWjxw5Mqz9p0H3BxEpkQIiEqGAiEQoICIRCohIhFaxBmloaAjWOzo6gvXJkyeXs52gXL2cPHkyWJ87d26wfu7cuSG1Wl6VGy6tYomUSAERiVBARCIUEJEIBUQkQnOxBvnqq6+C9ZUrVwbrCxYsCNbfeuutIbVcf5WXy6FDh4L15ubmYP3s2bPB+jXXXBOsr1ixYlj9XIh0BBGJUEBEIhQQkQgFRCRCARGJyHstlpltBhYAx919RlJrALYDk4BPgHvcPfynYj/eV+avxRquSy65JFgPzYXauHFjcNslS5YE6/fdd1+wvm3btgK7k5i0rsV6Bpg3qLYK2OPuU4E9yXORupM3IMko0cFfDiwEtiSPtwC3p9yXSCYU+0Vho7t3A7h7dzKbNygZah0cbC2SdWX/Jl3Dq6WWFRuQY2Y2Ljl6jAOOp9lULTl16lTB23799dfD2vfSpUuD9e3btwfroTE+Uppil3nbgUXJ40XAjnTaEcmWvAExs23Av4FpZtZlZkuAtUCzmX0INCfPRepO3lMsdw/frhRuSbkXkczRN+kiEQqISITG/lTQqFGjgvWXX345WL/pppuC9fnz5wfru3fvLq6xC5TG/oiUSAERiVBARCIUEJEIBUQkQqtYGTBlypRg/eDBg8F6riHV+/btC9Y7OzuD9Q0bNgypVfL/h2rTKpZIiRQQkQgFRCRCARGJUEBEIrSKlWEtLS3B+tNPPx2sjx49elj7X7169ZDa1q1bg9t2d3cPa9+1QKtYIiVSQEQiFBCRCAVEJEIBEYkodnj1w8BS4Itks9Xu/kreF9MqVipmzJgRrD/xxBPB+i23FD5fI9eA7TVr1gTrn3/+ecH7zppyDq8G+IO7NyX/5A2HSC0qdni1yAWhlM8gbWb2jpltNrOf59rIzJaZWaeZha+5FsmwYgPyJ2AK0AR0A4/n2tDdN7n7THefWeRriVRNUQFx92Puft7de4E/A7PSbUskGwq6FsvMJgE7B6xijeu/P4iZ/Q6Y7e73FrAfrWKV0ZgxY4L12267LVgPXdNlFl7Y2bt3b7De3NxcYHfZU8gqVt7ZvMnw6puBy8ysC3gIuNnMmgCn7x6F95fUqUhGFTu8+q9l6EUkc/RNukiEAiISoYCIROgvCi9g33///ZDaRReFP5b29PQE67feemuw/tprrxXdV6XoLwpFSqSAiEQoICIRCohIRN4vCiV7rr322mD9rrvuCtavu+66YD3XB/KQw4cPB+sHDhwoeB+1SEcQkQgFRCRCARGJUEBEIhQQkQitYmXAtGnTgvW2trZg/Y477gjWx44dW3Iv58+fD9ZzDa/u7e0t+TWzTEcQkQgFRCRCARGJUEBEIhQQkYhCpppMALYCY4FeYJO7P2lmDcB2YBJ9k03ucfcT5Wu1toRWlFpbQ/Mvcq9WTZo0Kc2WhujsHDrsMteQ6vb29rL2klWFHEF6gAfd/WrgemC5mU0HVgF73H0qsCd5LlJXChle3e3uB5PHp4EPgPHAQmBLstkW4PZyNSlSLcP6ojCZsPgroANo7J+u6O7dZnZ5jt9ZBiwrrU2R6ig4IGb2M+AF4AF3P5VrROVg7r4J2JTsQ0MbpKYUtIplZj+lLxzPuvuLSfmYmY1L/v044Hh5WhSpnkJWsYy+UaMfuPvAe3y1A4uAtcnPHWXpMCMaGxuD9enTpwfrTz311JDaVVddlWpPg3V0dATrjz32WLC+Y8fQ/2T1fm3VcBVyinUj8FvgXTM7lNRW0xeM581sCfApcHd5WhSpnkKGV/8LyPWBo/C7Q4rUIH2TLhKhgIhEKCAiERfsXxQ2NDQE6xs3bgzWm5qagvXJkyen1tNgr7/+erD++OPhe6bu2rUrWP/2229T6+lCoyOISIQCIhKhgIhEKCAiEQqISETdrGLNnj07WF+5cmWwPmvWrGB9/PjxqfU02DfffBOsr1+/Plh/9NFHg/WzZ8+m1pPE6QgiEqGAiEQoICIRCohIhAIiElE3q1gtLS3Dqg9Xrnv07dy5M1jv6ekZUst1DdXJkyeLb0zKSkcQkQgFRCRCARGJUEBEIsw9PsstMrz6YWAp8EWy6Wp3fyXPvjQ4TjLD3fNOPywkIOOAce5+0MxGA2/SN4f3HuCMu68rtCEFRLKkkIAUMvanG+ifwXvazPqHV4vUvWF9Bhk0vBqgzczeMbPNZvbzHL+zzMw6zWzozShEMi7vKdYPG/YNr94PrHH3F82sEfgScOD39J2GLc6zD51iSWak8hkEfhhevRPYNWg+b/+/nwTsdPcZefajgEhmFBKQvKdYuYZX9092T7QA7xXTpEiWFbKKNQf4J/Aufcu80De8uhVoou8U6xPg/v4b6kT2pSOIZEZqp1hpUUAkS1I5xRK5kCkgIhEKiEiEAiISoYCIRCggIhEKiEiEAiISoYCIRFR67M+XwP+Sx5clz+ud3mc2/aKQjSp6qcmPXtis091nVuXFK0jvs7bpFEskQgERiahmQDZV8bUrSe+zhlXtM4hILdAplkiEAiISUfGAmNk8MztiZh+Z2apKv345JeOPjpvZewNqDWb2qpl9mPwMjkeqJWY2wcz2mdkHZva+ma1I6nX3XisaEDMbAWwA5gPTgVYzm17JHsrsGWDeoNoqYI+7TwX2JM9rXQ/woLtfDVwPLE/+O9bde630EWQW8JG7f+zu54DngIUV7qFs3P0A8NWg8kJgS/J4C31jW2uau3e7+8Hk8Wmgf9pm3b3XSgdkPPDZgOdd1P8Y08b+aS/Jz8ur3E+qBk3brLv3WumAhKZIaJ25RiXTNl8AHnD3U9XupxwqHZAuYMKA51cARyvcQ6Ud6x+yl/w8XuV+UpFM23wBeNbdX0zKdfdeKx2QN4CpZnalmY0E7gXaK9xDpbUDi5LHi4AdVewlFbmmbVKP77XS36Sb2W+APwIjgM3uvqaiDZSRmW0Dbqbv0u9jwEPA34HngYnAp8Dd7j74g3xNiUzb7KDe3qsuNRHJTd+ki0QoICIRCohIhAIiEqGAiEQoICIRCohIxP8B5myH3fq0bdsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images = images.numpy()\n",
    "\n",
    "#get one image from batch\n",
    "img =  np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # define hidden linear layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        \n",
    "        # final fully-connected layer\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # flatten image\n",
    "        #x = x.reshape(1, -1)\n",
    "        #x = x.squeeze()\n",
    "        x = x.view(-1, 28*28)\n",
    "        # all hidden layers\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        # final layer\n",
    "        out = self.fc4(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # define hidden linear layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
    "        \n",
    "        # final fully-connected layer\n",
    "        self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # all hidden layers\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        # final layer with tanh applied\n",
    "        out = F.tanh(self.fc4(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Hyperparameters\n",
    "#size of the input image 28*28\n",
    "input_size = 784\n",
    "#size of the discriminator output (real or fake)\n",
    "d_output_size = 1\n",
    "#size of the last hidden layer in the discriminator\n",
    "d_hidden_size = 32\n",
    "\n",
    "# Generator Hyperparameters\n",
    "#size of the laten vector to give to generator\n",
    "z_size = 100\n",
    "#size of discriminator output (generator image)\n",
    "g_output_size = 784\n",
    "#size of first hidden layer in the generator\n",
    "g_hidden_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Complete Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (fc1): Linear(in_features=100, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=784, bias=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate discriminator and generator\n",
    "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
    "G = Generator(z_size, g_hidden_size, g_output_size)\n",
    "\n",
    "# check that they are as you expect\n",
    "print(D)\n",
    "print()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_loss = d_real_loss + d_fake_loss\n",
    "D(real_images) = 1 #label 1\n",
    "D(fake_images) = 0 #label 0\n",
    "\n",
    "Generalizing better, the labels are reducing a bit from 1 to 0.9\n",
    "labels = torch.ones(size)*0.9\n",
    "\n",
    "The discriminator loss for the fake data is similar. We want D(fake_images) = 0, where the fake images are the generator output fake_images = G(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetive: Get D(fake_images) = 1\n",
    "In this case, the labels are flipped to represent that the generator is trying to fool the discriminator into thinking that the images it generates (fakes) are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "        \n",
    "    # numerically stable loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Optimizaer hyperparameters\n",
    "lr = 0.002\n",
    "# create optimizers for the discriminator and generator\n",
    "d_optimizer = optim.Adam(D.parameters(), lr)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n",
      "Epoch [    1/  100] | d_loss: 1.2146 | g_loss: 1.3028\n",
      "Epoch [    1/  100] | d_loss: 1.3286 | g_loss: 0.9299\n",
      "Epoch [    1/  100] | d_loss: 1.3791 | g_loss: 1.0073\n",
      "Epoch [    2/  100] | d_loss: 1.3490 | g_loss: 1.2013\n",
      "Epoch [    2/  100] | d_loss: 1.2762 | g_loss: 0.9018\n",
      "Epoch [    2/  100] | d_loss: 1.4529 | g_loss: 1.0682\n",
      "Epoch [    3/  100] | d_loss: 1.2392 | g_loss: 1.1219\n",
      "Epoch [    3/  100] | d_loss: 1.2716 | g_loss: 0.9795\n",
      "Epoch [    3/  100] | d_loss: 1.3352 | g_loss: 1.0906\n",
      "Epoch [    4/  100] | d_loss: 1.2120 | g_loss: 1.3303\n",
      "Epoch [    4/  100] | d_loss: 1.2718 | g_loss: 1.7070\n",
      "Epoch [    4/  100] | d_loss: 1.2409 | g_loss: 1.0581\n",
      "Epoch [    5/  100] | d_loss: 1.1766 | g_loss: 1.4765\n",
      "Epoch [    5/  100] | d_loss: 1.2347 | g_loss: 1.0713\n",
      "Epoch [    5/  100] | d_loss: 1.2443 | g_loss: 1.2500\n",
      "Epoch [    6/  100] | d_loss: 1.2955 | g_loss: 1.0826\n",
      "Epoch [    6/  100] | d_loss: 1.2235 | g_loss: 1.1158\n",
      "Epoch [    6/  100] | d_loss: 1.3753 | g_loss: 0.9541\n",
      "Epoch [    7/  100] | d_loss: 1.2789 | g_loss: 1.0205\n",
      "Epoch [    7/  100] | d_loss: 1.0924 | g_loss: 1.5310\n",
      "Epoch [    7/  100] | d_loss: 1.3124 | g_loss: 0.9838\n",
      "Epoch [    8/  100] | d_loss: 1.3755 | g_loss: 1.4797\n",
      "Epoch [    8/  100] | d_loss: 1.2544 | g_loss: 0.9129\n",
      "Epoch [    8/  100] | d_loss: 1.4329 | g_loss: 0.9953\n",
      "Epoch [    9/  100] | d_loss: 1.2836 | g_loss: 1.4284\n",
      "Epoch [    9/  100] | d_loss: 1.2386 | g_loss: 1.1623\n",
      "Epoch [    9/  100] | d_loss: 1.3249 | g_loss: 0.9203\n",
      "Epoch [   10/  100] | d_loss: 1.3725 | g_loss: 1.0802\n",
      "Epoch [   10/  100] | d_loss: 1.2750 | g_loss: 0.8356\n",
      "Epoch [   10/  100] | d_loss: 1.3648 | g_loss: 0.9581\n",
      "Epoch [   11/  100] | d_loss: 1.2357 | g_loss: 1.1639\n",
      "Epoch [   11/  100] | d_loss: 1.3505 | g_loss: 1.0752\n",
      "Epoch [   11/  100] | d_loss: 1.3892 | g_loss: 0.9921\n",
      "Epoch [   12/  100] | d_loss: 1.3154 | g_loss: 0.8758\n",
      "Epoch [   12/  100] | d_loss: 1.2785 | g_loss: 0.9941\n",
      "Epoch [   12/  100] | d_loss: 1.2438 | g_loss: 1.1905\n",
      "Epoch [   13/  100] | d_loss: 1.3221 | g_loss: 1.0787\n",
      "Epoch [   13/  100] | d_loss: 1.2242 | g_loss: 1.2519\n",
      "Epoch [   13/  100] | d_loss: 1.3432 | g_loss: 1.1161\n",
      "Epoch [   14/  100] | d_loss: 1.3082 | g_loss: 0.9791\n",
      "Epoch [   14/  100] | d_loss: 1.2814 | g_loss: 1.0742\n",
      "Epoch [   14/  100] | d_loss: 1.3097 | g_loss: 0.9261\n",
      "Epoch [   15/  100] | d_loss: 1.1927 | g_loss: 1.0787\n",
      "Epoch [   15/  100] | d_loss: 1.2974 | g_loss: 1.0514\n",
      "Epoch [   15/  100] | d_loss: 1.2964 | g_loss: 1.2043\n",
      "Epoch [   16/  100] | d_loss: 1.2759 | g_loss: 0.8887\n",
      "Epoch [   16/  100] | d_loss: 1.3264 | g_loss: 0.8822\n",
      "Epoch [   16/  100] | d_loss: 1.3073 | g_loss: 1.1427\n",
      "Epoch [   17/  100] | d_loss: 1.2203 | g_loss: 1.2653\n",
      "Epoch [   17/  100] | d_loss: 1.2039 | g_loss: 1.0495\n",
      "Epoch [   17/  100] | d_loss: 1.4752 | g_loss: 0.9815\n",
      "Epoch [   18/  100] | d_loss: 1.3009 | g_loss: 1.1961\n",
      "Epoch [   18/  100] | d_loss: 1.4189 | g_loss: 0.8982\n",
      "Epoch [   18/  100] | d_loss: 1.3144 | g_loss: 0.9720\n",
      "Epoch [   19/  100] | d_loss: 1.2867 | g_loss: 0.9163\n",
      "Epoch [   19/  100] | d_loss: 1.2732 | g_loss: 0.9174\n",
      "Epoch [   19/  100] | d_loss: 1.3436 | g_loss: 1.0437\n",
      "Epoch [   20/  100] | d_loss: 1.2919 | g_loss: 0.9599\n",
      "Epoch [   20/  100] | d_loss: 1.3529 | g_loss: 0.8947\n",
      "Epoch [   20/  100] | d_loss: 1.2441 | g_loss: 1.0099\n",
      "Epoch [   21/  100] | d_loss: 1.2613 | g_loss: 0.8299\n",
      "Epoch [   21/  100] | d_loss: 1.1881 | g_loss: 1.0538\n",
      "Epoch [   21/  100] | d_loss: 1.3230 | g_loss: 0.9904\n",
      "Epoch [   22/  100] | d_loss: 1.3098 | g_loss: 0.9232\n",
      "Epoch [   22/  100] | d_loss: 1.1900 | g_loss: 1.0393\n",
      "Epoch [   22/  100] | d_loss: 1.3455 | g_loss: 1.1603\n",
      "Epoch [   23/  100] | d_loss: 1.2346 | g_loss: 1.5313\n",
      "Epoch [   23/  100] | d_loss: 1.2203 | g_loss: 1.0989\n",
      "Epoch [   23/  100] | d_loss: 1.3614 | g_loss: 1.0419\n",
      "Epoch [   24/  100] | d_loss: 1.1651 | g_loss: 1.0850\n",
      "Epoch [   24/  100] | d_loss: 1.2571 | g_loss: 1.0450\n",
      "Epoch [   24/  100] | d_loss: 1.4303 | g_loss: 1.3131\n",
      "Epoch [   25/  100] | d_loss: 1.3112 | g_loss: 1.1122\n",
      "Epoch [   25/  100] | d_loss: 1.2995 | g_loss: 0.9562\n",
      "Epoch [   25/  100] | d_loss: 1.3269 | g_loss: 0.9871\n",
      "Epoch [   26/  100] | d_loss: 1.3781 | g_loss: 0.9070\n",
      "Epoch [   26/  100] | d_loss: 1.3079 | g_loss: 0.9837\n",
      "Epoch [   26/  100] | d_loss: 1.4194 | g_loss: 1.1546\n",
      "Epoch [   27/  100] | d_loss: 1.2806 | g_loss: 0.9729\n",
      "Epoch [   27/  100] | d_loss: 1.4162 | g_loss: 0.9535\n",
      "Epoch [   27/  100] | d_loss: 1.2874 | g_loss: 1.3391\n",
      "Epoch [   28/  100] | d_loss: 1.2573 | g_loss: 1.4056\n",
      "Epoch [   28/  100] | d_loss: 1.3294 | g_loss: 1.1258\n",
      "Epoch [   28/  100] | d_loss: 1.3535 | g_loss: 0.9091\n",
      "Epoch [   29/  100] | d_loss: 1.3611 | g_loss: 2.0905\n",
      "Epoch [   29/  100] | d_loss: 1.2422 | g_loss: 0.9329\n",
      "Epoch [   29/  100] | d_loss: 1.4671 | g_loss: 0.9407\n",
      "Epoch [   30/  100] | d_loss: 1.2110 | g_loss: 1.4235\n",
      "Epoch [   30/  100] | d_loss: 1.2640 | g_loss: 1.1131\n",
      "Epoch [   30/  100] | d_loss: 1.3960 | g_loss: 0.9974\n",
      "Epoch [   31/  100] | d_loss: 1.2912 | g_loss: 1.0177\n",
      "Epoch [   31/  100] | d_loss: 1.2414 | g_loss: 1.0965\n",
      "Epoch [   31/  100] | d_loss: 1.2282 | g_loss: 0.8831\n",
      "Epoch [   32/  100] | d_loss: 1.3275 | g_loss: 1.4682\n",
      "Epoch [   32/  100] | d_loss: 1.3663 | g_loss: 0.9926\n",
      "Epoch [   32/  100] | d_loss: 1.3834 | g_loss: 0.7776\n",
      "Epoch [   33/  100] | d_loss: 1.2576 | g_loss: 1.1051\n",
      "Epoch [   33/  100] | d_loss: 1.3253 | g_loss: 0.9243\n",
      "Epoch [   33/  100] | d_loss: 1.2964 | g_loss: 1.1909\n",
      "Epoch [   34/  100] | d_loss: 1.2519 | g_loss: 1.5134\n",
      "Epoch [   34/  100] | d_loss: 1.1826 | g_loss: 1.0891\n",
      "Epoch [   34/  100] | d_loss: 1.2595 | g_loss: 1.0262\n",
      "Epoch [   35/  100] | d_loss: 1.3350 | g_loss: 0.9531\n",
      "Epoch [   35/  100] | d_loss: 1.1881 | g_loss: 1.1847\n",
      "Epoch [   35/  100] | d_loss: 1.3496 | g_loss: 0.9418\n",
      "Epoch [   36/  100] | d_loss: 1.2873 | g_loss: 1.3337\n",
      "Epoch [   36/  100] | d_loss: 1.2465 | g_loss: 1.0181\n",
      "Epoch [   36/  100] | d_loss: 1.3081 | g_loss: 1.1227\n",
      "Epoch [   37/  100] | d_loss: 1.3304 | g_loss: 1.2942\n",
      "Epoch [   37/  100] | d_loss: 1.3788 | g_loss: 1.0118\n",
      "Epoch [   37/  100] | d_loss: 1.3970 | g_loss: 0.8965\n",
      "Epoch [   38/  100] | d_loss: 1.2330 | g_loss: 1.2983\n",
      "Epoch [   38/  100] | d_loss: 1.2627 | g_loss: 0.8980\n",
      "Epoch [   38/  100] | d_loss: 1.3644 | g_loss: 0.9003\n",
      "Epoch [   39/  100] | d_loss: 1.2581 | g_loss: 0.8993\n",
      "Epoch [   39/  100] | d_loss: 1.2859 | g_loss: 1.1251\n",
      "Epoch [   39/  100] | d_loss: 1.3318 | g_loss: 1.1394\n",
      "Epoch [   40/  100] | d_loss: 1.3007 | g_loss: 0.9899\n",
      "Epoch [   40/  100] | d_loss: 1.2400 | g_loss: 0.9745\n",
      "Epoch [   40/  100] | d_loss: 1.4139 | g_loss: 1.0698\n",
      "Epoch [   41/  100] | d_loss: 1.2211 | g_loss: 1.0893\n",
      "Epoch [   41/  100] | d_loss: 1.2892 | g_loss: 1.0507\n",
      "Epoch [   41/  100] | d_loss: 1.3465 | g_loss: 1.0897\n",
      "Epoch [   42/  100] | d_loss: 1.2328 | g_loss: 1.1982\n",
      "Epoch [   42/  100] | d_loss: 1.1954 | g_loss: 1.0781\n",
      "Epoch [   42/  100] | d_loss: 1.4581 | g_loss: 0.9712\n",
      "Epoch [   43/  100] | d_loss: 1.2819 | g_loss: 0.9848\n",
      "Epoch [   43/  100] | d_loss: 1.3235 | g_loss: 0.9305\n",
      "Epoch [   43/  100] | d_loss: 1.3913 | g_loss: 0.9595\n",
      "Epoch [   44/  100] | d_loss: 1.2630 | g_loss: 1.1821\n",
      "Epoch [   44/  100] | d_loss: 1.2722 | g_loss: 0.9377\n",
      "Epoch [   44/  100] | d_loss: 1.2757 | g_loss: 0.9563\n",
      "Epoch [   45/  100] | d_loss: 1.4644 | g_loss: 1.4129\n",
      "Epoch [   45/  100] | d_loss: 1.2714 | g_loss: 1.0010\n",
      "Epoch [   45/  100] | d_loss: 1.4872 | g_loss: 1.0486\n",
      "Epoch [   46/  100] | d_loss: 1.2412 | g_loss: 1.1938\n",
      "Epoch [   46/  100] | d_loss: 1.2876 | g_loss: 1.1138\n",
      "Epoch [   46/  100] | d_loss: 1.4585 | g_loss: 0.9521\n",
      "Epoch [   47/  100] | d_loss: 1.2530 | g_loss: 1.0320\n",
      "Epoch [   47/  100] | d_loss: 1.2626 | g_loss: 1.0003\n",
      "Epoch [   47/  100] | d_loss: 1.3066 | g_loss: 1.0505\n",
      "Epoch [   48/  100] | d_loss: 1.3677 | g_loss: 1.3128\n",
      "Epoch [   48/  100] | d_loss: 1.2398 | g_loss: 1.0128\n",
      "Epoch [   48/  100] | d_loss: 1.3253 | g_loss: 1.0522\n",
      "Epoch [   49/  100] | d_loss: 1.2966 | g_loss: 1.0506\n",
      "Epoch [   49/  100] | d_loss: 1.1274 | g_loss: 1.1292\n",
      "Epoch [   49/  100] | d_loss: 1.3045 | g_loss: 0.9240\n",
      "Epoch [   50/  100] | d_loss: 1.3556 | g_loss: 1.2225\n",
      "Epoch [   50/  100] | d_loss: 1.2871 | g_loss: 0.9586\n",
      "Epoch [   50/  100] | d_loss: 1.1908 | g_loss: 1.2666\n",
      "Epoch [   51/  100] | d_loss: 1.3455 | g_loss: 1.0578\n",
      "Epoch [   51/  100] | d_loss: 1.2191 | g_loss: 1.0674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   51/  100] | d_loss: 1.3187 | g_loss: 1.0293\n",
      "Epoch [   52/  100] | d_loss: 1.3819 | g_loss: 1.3321\n",
      "Epoch [   52/  100] | d_loss: 1.1516 | g_loss: 1.0726\n",
      "Epoch [   52/  100] | d_loss: 1.3005 | g_loss: 1.1252\n",
      "Epoch [   53/  100] | d_loss: 1.4706 | g_loss: 1.1909\n",
      "Epoch [   53/  100] | d_loss: 1.2540 | g_loss: 1.0873\n",
      "Epoch [   53/  100] | d_loss: 1.3990 | g_loss: 1.0827\n",
      "Epoch [   54/  100] | d_loss: 1.3057 | g_loss: 0.9888\n",
      "Epoch [   54/  100] | d_loss: 1.2841 | g_loss: 0.9245\n",
      "Epoch [   54/  100] | d_loss: 1.3524 | g_loss: 0.9896\n",
      "Epoch [   55/  100] | d_loss: 1.3228 | g_loss: 1.2409\n",
      "Epoch [   55/  100] | d_loss: 1.2539 | g_loss: 0.9310\n",
      "Epoch [   55/  100] | d_loss: 1.3772 | g_loss: 1.5121\n",
      "Epoch [   56/  100] | d_loss: 1.3317 | g_loss: 1.2715\n",
      "Epoch [   56/  100] | d_loss: 1.2990 | g_loss: 0.9543\n",
      "Epoch [   56/  100] | d_loss: 1.3368 | g_loss: 1.0591\n",
      "Epoch [   57/  100] | d_loss: 1.2724 | g_loss: 1.2942\n",
      "Epoch [   57/  100] | d_loss: 1.3445 | g_loss: 1.0290\n",
      "Epoch [   57/  100] | d_loss: 1.2875 | g_loss: 1.0104\n",
      "Epoch [   58/  100] | d_loss: 1.2190 | g_loss: 1.1993\n",
      "Epoch [   58/  100] | d_loss: 1.1460 | g_loss: 0.9663\n",
      "Epoch [   58/  100] | d_loss: 1.2370 | g_loss: 0.9947\n",
      "Epoch [   59/  100] | d_loss: 1.2640 | g_loss: 1.0758\n",
      "Epoch [   59/  100] | d_loss: 1.2154 | g_loss: 1.2106\n",
      "Epoch [   59/  100] | d_loss: 1.3759 | g_loss: 0.9861\n",
      "Epoch [   60/  100] | d_loss: 1.3029 | g_loss: 0.8512\n",
      "Epoch [   60/  100] | d_loss: 1.2926 | g_loss: 0.9999\n",
      "Epoch [   60/  100] | d_loss: 1.4135 | g_loss: 0.8768\n",
      "Epoch [   61/  100] | d_loss: 1.1154 | g_loss: 1.0732\n",
      "Epoch [   61/  100] | d_loss: 1.2412 | g_loss: 1.1192\n",
      "Epoch [   61/  100] | d_loss: 1.4757 | g_loss: 1.1847\n",
      "Epoch [   62/  100] | d_loss: 1.3095 | g_loss: 1.2879\n",
      "Epoch [   62/  100] | d_loss: 1.1942 | g_loss: 1.0668\n",
      "Epoch [   62/  100] | d_loss: 1.2953 | g_loss: 1.0723\n",
      "Epoch [   63/  100] | d_loss: 1.3030 | g_loss: 1.2454\n",
      "Epoch [   63/  100] | d_loss: 1.2576 | g_loss: 0.9942\n",
      "Epoch [   63/  100] | d_loss: 1.3572 | g_loss: 1.1849\n",
      "Epoch [   64/  100] | d_loss: 1.3519 | g_loss: 1.0554\n",
      "Epoch [   64/  100] | d_loss: 1.2195 | g_loss: 1.0972\n",
      "Epoch [   64/  100] | d_loss: 1.2583 | g_loss: 1.2048\n",
      "Epoch [   65/  100] | d_loss: 1.2296 | g_loss: 1.2822\n",
      "Epoch [   65/  100] | d_loss: 1.1910 | g_loss: 1.2859\n",
      "Epoch [   65/  100] | d_loss: 1.3183 | g_loss: 0.9824\n",
      "Epoch [   66/  100] | d_loss: 1.2457 | g_loss: 1.0846\n",
      "Epoch [   66/  100] | d_loss: 1.2216 | g_loss: 0.9978\n",
      "Epoch [   66/  100] | d_loss: 1.3009 | g_loss: 0.9645\n",
      "Epoch [   67/  100] | d_loss: 1.3677 | g_loss: 0.9942\n",
      "Epoch [   67/  100] | d_loss: 1.2444 | g_loss: 0.9643\n",
      "Epoch [   67/  100] | d_loss: 1.3697 | g_loss: 1.1507\n",
      "Epoch [   68/  100] | d_loss: 1.2896 | g_loss: 1.1473\n",
      "Epoch [   68/  100] | d_loss: 1.1815 | g_loss: 1.0907\n",
      "Epoch [   68/  100] | d_loss: 1.3639 | g_loss: 1.0455\n",
      "Epoch [   69/  100] | d_loss: 1.3263 | g_loss: 0.9978\n",
      "Epoch [   69/  100] | d_loss: 1.2641 | g_loss: 1.0487\n",
      "Epoch [   69/  100] | d_loss: 1.3261 | g_loss: 0.8840\n",
      "Epoch [   70/  100] | d_loss: 1.1785 | g_loss: 1.2002\n",
      "Epoch [   70/  100] | d_loss: 1.1698 | g_loss: 1.1298\n",
      "Epoch [   70/  100] | d_loss: 1.3023 | g_loss: 0.9821\n",
      "Epoch [   71/  100] | d_loss: 1.2864 | g_loss: 0.9845\n",
      "Epoch [   71/  100] | d_loss: 1.1431 | g_loss: 1.0871\n",
      "Epoch [   71/  100] | d_loss: 1.3555 | g_loss: 1.0619\n",
      "Epoch [   72/  100] | d_loss: 1.2097 | g_loss: 1.1399\n",
      "Epoch [   72/  100] | d_loss: 1.3208 | g_loss: 0.9273\n",
      "Epoch [   72/  100] | d_loss: 1.3057 | g_loss: 0.9165\n",
      "Epoch [   73/  100] | d_loss: 1.2943 | g_loss: 1.0126\n",
      "Epoch [   73/  100] | d_loss: 1.1598 | g_loss: 1.0576\n",
      "Epoch [   73/  100] | d_loss: 1.3004 | g_loss: 1.0800\n",
      "Epoch [   74/  100] | d_loss: 1.2353 | g_loss: 1.0441\n",
      "Epoch [   74/  100] | d_loss: 1.2218 | g_loss: 1.0528\n",
      "Epoch [   74/  100] | d_loss: 1.2746 | g_loss: 1.0684\n",
      "Epoch [   75/  100] | d_loss: 1.3532 | g_loss: 1.6112\n",
      "Epoch [   75/  100] | d_loss: 1.3842 | g_loss: 0.9853\n",
      "Epoch [   75/  100] | d_loss: 1.3899 | g_loss: 1.3204\n",
      "Epoch [   76/  100] | d_loss: 1.3330 | g_loss: 1.3863\n",
      "Epoch [   76/  100] | d_loss: 1.2695 | g_loss: 1.0249\n",
      "Epoch [   76/  100] | d_loss: 1.2810 | g_loss: 0.9869\n",
      "Epoch [   77/  100] | d_loss: 1.2450 | g_loss: 1.3302\n",
      "Epoch [   77/  100] | d_loss: 1.2683 | g_loss: 1.1905\n",
      "Epoch [   77/  100] | d_loss: 1.3707 | g_loss: 0.9830\n",
      "Epoch [   78/  100] | d_loss: 1.2992 | g_loss: 1.0162\n",
      "Epoch [   78/  100] | d_loss: 1.3124 | g_loss: 1.1236\n",
      "Epoch [   78/  100] | d_loss: 1.2810 | g_loss: 1.1679\n",
      "Epoch [   79/  100] | d_loss: 1.3631 | g_loss: 0.9569\n",
      "Epoch [   79/  100] | d_loss: 1.3296 | g_loss: 1.1991\n",
      "Epoch [   79/  100] | d_loss: 1.2898 | g_loss: 1.1853\n",
      "Epoch [   80/  100] | d_loss: 1.1655 | g_loss: 1.0875\n",
      "Epoch [   80/  100] | d_loss: 1.2148 | g_loss: 0.9268\n",
      "Epoch [   80/  100] | d_loss: 1.4294 | g_loss: 1.1507\n",
      "Epoch [   81/  100] | d_loss: 1.2742 | g_loss: 0.8302\n",
      "Epoch [   81/  100] | d_loss: 1.2255 | g_loss: 1.0113\n",
      "Epoch [   81/  100] | d_loss: 1.4850 | g_loss: 1.3411\n",
      "Epoch [   82/  100] | d_loss: 1.3172 | g_loss: 1.2924\n",
      "Epoch [   82/  100] | d_loss: 1.1605 | g_loss: 1.0034\n",
      "Epoch [   82/  100] | d_loss: 1.3708 | g_loss: 1.0326\n",
      "Epoch [   83/  100] | d_loss: 1.1807 | g_loss: 1.0248\n",
      "Epoch [   83/  100] | d_loss: 1.1977 | g_loss: 1.2338\n",
      "Epoch [   83/  100] | d_loss: 1.3914 | g_loss: 0.9822\n",
      "Epoch [   84/  100] | d_loss: 1.3211 | g_loss: 1.0893\n",
      "Epoch [   84/  100] | d_loss: 1.2285 | g_loss: 1.0286\n",
      "Epoch [   84/  100] | d_loss: 1.3059 | g_loss: 0.9876\n",
      "Epoch [   85/  100] | d_loss: 1.3903 | g_loss: 1.0317\n",
      "Epoch [   85/  100] | d_loss: 1.1775 | g_loss: 1.4776\n",
      "Epoch [   85/  100] | d_loss: 1.3250 | g_loss: 0.9820\n",
      "Epoch [   86/  100] | d_loss: 1.3121 | g_loss: 0.9869\n",
      "Epoch [   86/  100] | d_loss: 1.2555 | g_loss: 1.1241\n",
      "Epoch [   86/  100] | d_loss: 1.3844 | g_loss: 1.0886\n",
      "Epoch [   87/  100] | d_loss: 1.3826 | g_loss: 1.0500\n",
      "Epoch [   87/  100] | d_loss: 1.2461 | g_loss: 1.5649\n",
      "Epoch [   87/  100] | d_loss: 1.2900 | g_loss: 1.1976\n",
      "Epoch [   88/  100] | d_loss: 1.3971 | g_loss: 1.1124\n",
      "Epoch [   88/  100] | d_loss: 1.3310 | g_loss: 1.3821\n",
      "Epoch [   88/  100] | d_loss: 1.4484 | g_loss: 1.1451\n",
      "Epoch [   89/  100] | d_loss: 1.3655 | g_loss: 1.1882\n",
      "Epoch [   89/  100] | d_loss: 1.2192 | g_loss: 1.0006\n",
      "Epoch [   89/  100] | d_loss: 1.3046 | g_loss: 0.8629\n",
      "Epoch [   90/  100] | d_loss: 1.2754 | g_loss: 1.0379\n",
      "Epoch [   90/  100] | d_loss: 1.3254 | g_loss: 1.1155\n",
      "Epoch [   90/  100] | d_loss: 1.3120 | g_loss: 1.6867\n",
      "Epoch [   91/  100] | d_loss: 1.3066 | g_loss: 0.9675\n",
      "Epoch [   91/  100] | d_loss: 1.3378 | g_loss: 1.1083\n",
      "Epoch [   91/  100] | d_loss: 1.3420 | g_loss: 1.2776\n",
      "Epoch [   92/  100] | d_loss: 1.1956 | g_loss: 1.2366\n",
      "Epoch [   92/  100] | d_loss: 1.2548 | g_loss: 1.3141\n",
      "Epoch [   92/  100] | d_loss: 1.4254 | g_loss: 0.9526\n",
      "Epoch [   93/  100] | d_loss: 1.2886 | g_loss: 1.4652\n",
      "Epoch [   93/  100] | d_loss: 1.2629 | g_loss: 1.2794\n",
      "Epoch [   93/  100] | d_loss: 1.3426 | g_loss: 0.8978\n",
      "Epoch [   94/  100] | d_loss: 1.1343 | g_loss: 1.3092\n",
      "Epoch [   94/  100] | d_loss: 1.2706 | g_loss: 1.0435\n",
      "Epoch [   94/  100] | d_loss: 1.4093 | g_loss: 1.0532\n",
      "Epoch [   95/  100] | d_loss: 1.4229 | g_loss: 0.8646\n",
      "Epoch [   95/  100] | d_loss: 1.2278 | g_loss: 1.0952\n",
      "Epoch [   95/  100] | d_loss: 1.3756 | g_loss: 0.9972\n",
      "Epoch [   96/  100] | d_loss: 1.2470 | g_loss: 1.0668\n",
      "Epoch [   96/  100] | d_loss: 1.1328 | g_loss: 1.0908\n",
      "Epoch [   96/  100] | d_loss: 1.4232 | g_loss: 1.2530\n",
      "Epoch [   97/  100] | d_loss: 1.3105 | g_loss: 1.5596\n",
      "Epoch [   97/  100] | d_loss: 1.2398 | g_loss: 0.9238\n",
      "Epoch [   97/  100] | d_loss: 1.2980 | g_loss: 1.0544\n",
      "Epoch [   98/  100] | d_loss: 1.2672 | g_loss: 1.1274\n",
      "Epoch [   98/  100] | d_loss: 1.2772 | g_loss: 1.0579\n",
      "Epoch [   98/  100] | d_loss: 1.3144 | g_loss: 0.9686\n",
      "Epoch [   99/  100] | d_loss: 1.2320 | g_loss: 1.4723\n",
      "Epoch [   99/  100] | d_loss: 1.1493 | g_loss: 1.0407\n",
      "Epoch [   99/  100] | d_loss: 1.2365 | g_loss: 1.0214\n",
      "Epoch [  100/  100] | d_loss: 1.2014 | g_loss: 1.0177\n",
      "Epoch [  100/  100] | d_loss: 1.2776 | g_loss: 1.0957\n",
      "Epoch [  100/  100] | d_loss: 1.3691 | g_loss: 0.9862\n",
      "elapsed_time = 21.0 min, 1297.7520608901978 sec\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"Training on GPU\")\n",
    "else:\n",
    "    print(\"Training on CPU\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# training hyperparams\n",
    "num_epochs = 100\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "print_every = 400\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance\n",
    "sample_size=16\n",
    "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "fixed_z = torch.from_numpy(fixed_z).float()\n",
    "\n",
    "# train the network\n",
    "D.train()\n",
    "G.train()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
    "                \n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        ## Important rescaling step ## \n",
    "        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n",
    "        \n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATOR\n",
    "        # ============================================\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Train with real images\n",
    "\n",
    "        # Compute the discriminator losses on real images \n",
    "        # smooth the real labels\n",
    "        D_real = D(real_images)\n",
    "        d_real_loss = real_loss(D_real, smooth=True)\n",
    "        \n",
    "        # 2. Train with fake images\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images        \n",
    "        D_fake = D(fake_images)\n",
    "        d_fake_loss = fake_loss(D_fake)\n",
    "        \n",
    "        # add up loss and perform backprop\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATOR\n",
    "        # =========================================\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Train with fake images and flipped labels\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images \n",
    "        # using flipped labels!\n",
    "        D_fake = D(fake_images)\n",
    "        g_loss = real_loss(D_fake) # use real loss to flip labels\n",
    "        \n",
    "        # perform backprop\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##\n",
    "    # append discriminator loss and generator loss\n",
    "    losses.append((d_loss.item(), g_loss.item()))\n",
    "    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # eval mode for generating samples\n",
    "    samples_z = G(fixed_z)\n",
    "    samples.append(samples_z)\n",
    "    G.train() # back to train mode\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"elapsed_time = {} min, {} sec\".format(elapsed_time//60, elapsed_time))\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (fc1): Linear(in_features=200, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=784, bias=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#config 2\n",
    "import torch.optim as optim\n",
    "# Optimizaer hyperparameters\n",
    "lr2 = 0.0002\n",
    "# create optimizers for the discriminator and generator\n",
    "d_optimizer2 = optim.Adam(D.parameters(), lr2)\n",
    "g_optimizer2 = optim.Adam(G.parameters(), lr2)\n",
    "\n",
    "\n",
    "# Discriminator Hyperparameters\n",
    "#size of the input image 28*28\n",
    "input_size2 = 784\n",
    "#size of the discriminator output (real or fake)\n",
    "d_output_size2 = 1\n",
    "#size of the last hidden layer in the discriminator\n",
    "d_hidden_size2 = 64\n",
    "\n",
    "# Generator Hyperparameters\n",
    "#size of the laten vector to give to generator\n",
    "z_size2 = 200\n",
    "#size of discriminator output (generator image)\n",
    "g_output_size2 = 784\n",
    "#size of first hidden layer in the generator\n",
    "g_hidden_size2 = 64\n",
    "\n",
    "# instantiate discriminator and generator\n",
    "D2 = Discriminator(input_size2, d_hidden_size2, d_output_size2)\n",
    "G2 = Generator(z_size2, g_hidden_size2, g_output_size2)\n",
    "\n",
    "# check that they are as you expect\n",
    "print(D2)\n",
    "print()\n",
    "print(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/  200] | d_loss: 1.4038 | g_loss: 0.6748\n",
      "Epoch [    1/  200] | d_loss: 1.4067 | g_loss: 0.6753\n",
      "Epoch [    1/  200] | d_loss: 1.3989 | g_loss: 0.6748\n",
      "Epoch [    2/  200] | d_loss: 1.3958 | g_loss: 0.6757\n",
      "Epoch [    2/  200] | d_loss: 1.4021 | g_loss: 0.6766\n",
      "Epoch [    2/  200] | d_loss: 1.4021 | g_loss: 0.6774\n",
      "Epoch [    3/  200] | d_loss: 1.4012 | g_loss: 0.6750\n",
      "Epoch [    3/  200] | d_loss: 1.3943 | g_loss: 0.6757\n",
      "Epoch [    3/  200] | d_loss: 1.4013 | g_loss: 0.6740\n",
      "Epoch [    4/  200] | d_loss: 1.4000 | g_loss: 0.6763\n",
      "Epoch [    4/  200] | d_loss: 1.3979 | g_loss: 0.6743\n",
      "Epoch [    4/  200] | d_loss: 1.4004 | g_loss: 0.6764\n",
      "Epoch [    5/  200] | d_loss: 1.3931 | g_loss: 0.6773\n",
      "Epoch [    5/  200] | d_loss: 1.4011 | g_loss: 0.6749\n",
      "Epoch [    5/  200] | d_loss: 1.4032 | g_loss: 0.6755\n",
      "Epoch [    6/  200] | d_loss: 1.4093 | g_loss: 0.6758\n",
      "Epoch [    6/  200] | d_loss: 1.4016 | g_loss: 0.6763\n",
      "Epoch [    6/  200] | d_loss: 1.4018 | g_loss: 0.6751\n",
      "Epoch [    7/  200] | d_loss: 1.3958 | g_loss: 0.6771\n",
      "Epoch [    7/  200] | d_loss: 1.4024 | g_loss: 0.6758\n",
      "Epoch [    7/  200] | d_loss: 1.4007 | g_loss: 0.6770\n",
      "Epoch [    8/  200] | d_loss: 1.4038 | g_loss: 0.6762\n",
      "Epoch [    8/  200] | d_loss: 1.4002 | g_loss: 0.6764\n",
      "Epoch [    8/  200] | d_loss: 1.4059 | g_loss: 0.6746\n",
      "Epoch [    9/  200] | d_loss: 1.4005 | g_loss: 0.6764\n",
      "Epoch [    9/  200] | d_loss: 1.3984 | g_loss: 0.6762\n",
      "Epoch [    9/  200] | d_loss: 1.4021 | g_loss: 0.6764\n",
      "Epoch [   10/  200] | d_loss: 1.3965 | g_loss: 0.6771\n",
      "Epoch [   10/  200] | d_loss: 1.3991 | g_loss: 0.6757\n",
      "Epoch [   10/  200] | d_loss: 1.3994 | g_loss: 0.6756\n",
      "Epoch [   11/  200] | d_loss: 1.3986 | g_loss: 0.6752\n",
      "Epoch [   11/  200] | d_loss: 1.4058 | g_loss: 0.6760\n",
      "Epoch [   11/  200] | d_loss: 1.3959 | g_loss: 0.6761\n",
      "Epoch [   12/  200] | d_loss: 1.3939 | g_loss: 0.6755\n",
      "Epoch [   12/  200] | d_loss: 1.4015 | g_loss: 0.6745\n",
      "Epoch [   12/  200] | d_loss: 1.3989 | g_loss: 0.6770\n",
      "Epoch [   13/  200] | d_loss: 1.3998 | g_loss: 0.6748\n",
      "Epoch [   13/  200] | d_loss: 1.4059 | g_loss: 0.6761\n",
      "Epoch [   13/  200] | d_loss: 1.4060 | g_loss: 0.6755\n",
      "Epoch [   14/  200] | d_loss: 1.3995 | g_loss: 0.6772\n",
      "Epoch [   14/  200] | d_loss: 1.4043 | g_loss: 0.6758\n",
      "Epoch [   14/  200] | d_loss: 1.3993 | g_loss: 0.6757\n",
      "Epoch [   15/  200] | d_loss: 1.4004 | g_loss: 0.6755\n",
      "Epoch [   15/  200] | d_loss: 1.4045 | g_loss: 0.6746\n",
      "Epoch [   15/  200] | d_loss: 1.3983 | g_loss: 0.6751\n",
      "Epoch [   16/  200] | d_loss: 1.4004 | g_loss: 0.6753\n",
      "Epoch [   16/  200] | d_loss: 1.4049 | g_loss: 0.6753\n",
      "Epoch [   16/  200] | d_loss: 1.4031 | g_loss: 0.6743\n",
      "Epoch [   17/  200] | d_loss: 1.4023 | g_loss: 0.6768\n",
      "Epoch [   17/  200] | d_loss: 1.4016 | g_loss: 0.6750\n",
      "Epoch [   17/  200] | d_loss: 1.3995 | g_loss: 0.6744\n",
      "Epoch [   18/  200] | d_loss: 1.4026 | g_loss: 0.6758\n",
      "Epoch [   18/  200] | d_loss: 1.4016 | g_loss: 0.6754\n",
      "Epoch [   18/  200] | d_loss: 1.4000 | g_loss: 0.6759\n",
      "Epoch [   19/  200] | d_loss: 1.4073 | g_loss: 0.6755\n",
      "Epoch [   19/  200] | d_loss: 1.4007 | g_loss: 0.6760\n",
      "Epoch [   19/  200] | d_loss: 1.3987 | g_loss: 0.6752\n",
      "Epoch [   20/  200] | d_loss: 1.4015 | g_loss: 0.6737\n",
      "Epoch [   20/  200] | d_loss: 1.4037 | g_loss: 0.6745\n",
      "Epoch [   20/  200] | d_loss: 1.4008 | g_loss: 0.6762\n",
      "Epoch [   21/  200] | d_loss: 1.4027 | g_loss: 0.6736\n",
      "Epoch [   21/  200] | d_loss: 1.4008 | g_loss: 0.6748\n",
      "Epoch [   21/  200] | d_loss: 1.3996 | g_loss: 0.6745\n",
      "Epoch [   22/  200] | d_loss: 1.4027 | g_loss: 0.6749\n",
      "Epoch [   22/  200] | d_loss: 1.4045 | g_loss: 0.6746\n",
      "Epoch [   22/  200] | d_loss: 1.4000 | g_loss: 0.6738\n",
      "Epoch [   23/  200] | d_loss: 1.4000 | g_loss: 0.6759\n",
      "Epoch [   23/  200] | d_loss: 1.4017 | g_loss: 0.6757\n",
      "Epoch [   23/  200] | d_loss: 1.4085 | g_loss: 0.6754\n",
      "Epoch [   24/  200] | d_loss: 1.4006 | g_loss: 0.6750\n",
      "Epoch [   24/  200] | d_loss: 1.4037 | g_loss: 0.6754\n",
      "Epoch [   24/  200] | d_loss: 1.4010 | g_loss: 0.6768\n",
      "Epoch [   25/  200] | d_loss: 1.4025 | g_loss: 0.6753\n",
      "Epoch [   25/  200] | d_loss: 1.4017 | g_loss: 0.6761\n",
      "Epoch [   25/  200] | d_loss: 1.4029 | g_loss: 0.6758\n",
      "Epoch [   26/  200] | d_loss: 1.3971 | g_loss: 0.6750\n",
      "Epoch [   26/  200] | d_loss: 1.4062 | g_loss: 0.6750\n",
      "Epoch [   26/  200] | d_loss: 1.4010 | g_loss: 0.6758\n",
      "Epoch [   27/  200] | d_loss: 1.3963 | g_loss: 0.6759\n",
      "Epoch [   27/  200] | d_loss: 1.4015 | g_loss: 0.6749\n",
      "Epoch [   27/  200] | d_loss: 1.3990 | g_loss: 0.6759\n",
      "Epoch [   28/  200] | d_loss: 1.4045 | g_loss: 0.6772\n",
      "Epoch [   28/  200] | d_loss: 1.4065 | g_loss: 0.6749\n",
      "Epoch [   28/  200] | d_loss: 1.4037 | g_loss: 0.6750\n",
      "Epoch [   29/  200] | d_loss: 1.3958 | g_loss: 0.6761\n",
      "Epoch [   29/  200] | d_loss: 1.4043 | g_loss: 0.6766\n",
      "Epoch [   29/  200] | d_loss: 1.4105 | g_loss: 0.6749\n",
      "Epoch [   30/  200] | d_loss: 1.4009 | g_loss: 0.6764\n",
      "Epoch [   30/  200] | d_loss: 1.4060 | g_loss: 0.6750\n",
      "Epoch [   30/  200] | d_loss: 1.4022 | g_loss: 0.6766\n",
      "Epoch [   31/  200] | d_loss: 1.4079 | g_loss: 0.6775\n",
      "Epoch [   31/  200] | d_loss: 1.4016 | g_loss: 0.6763\n",
      "Epoch [   31/  200] | d_loss: 1.3965 | g_loss: 0.6758\n",
      "Epoch [   32/  200] | d_loss: 1.4070 | g_loss: 0.6757\n",
      "Epoch [   32/  200] | d_loss: 1.4013 | g_loss: 0.6744\n",
      "Epoch [   32/  200] | d_loss: 1.4035 | g_loss: 0.6759\n",
      "Epoch [   33/  200] | d_loss: 1.3996 | g_loss: 0.6758\n",
      "Epoch [   33/  200] | d_loss: 1.4064 | g_loss: 0.6775\n",
      "Epoch [   33/  200] | d_loss: 1.3953 | g_loss: 0.6756\n",
      "Epoch [   34/  200] | d_loss: 1.3962 | g_loss: 0.6748\n",
      "Epoch [   34/  200] | d_loss: 1.4025 | g_loss: 0.6743\n",
      "Epoch [   34/  200] | d_loss: 1.4025 | g_loss: 0.6753\n",
      "Epoch [   35/  200] | d_loss: 1.3936 | g_loss: 0.6739\n",
      "Epoch [   35/  200] | d_loss: 1.3997 | g_loss: 0.6758\n",
      "Epoch [   35/  200] | d_loss: 1.4035 | g_loss: 0.6741\n",
      "Epoch [   36/  200] | d_loss: 1.4082 | g_loss: 0.6745\n",
      "Epoch [   36/  200] | d_loss: 1.3995 | g_loss: 0.6749\n",
      "Epoch [   36/  200] | d_loss: 1.4023 | g_loss: 0.6768\n",
      "Epoch [   37/  200] | d_loss: 1.3993 | g_loss: 0.6760\n",
      "Epoch [   37/  200] | d_loss: 1.4009 | g_loss: 0.6765\n",
      "Epoch [   37/  200] | d_loss: 1.4019 | g_loss: 0.6768\n",
      "Epoch [   38/  200] | d_loss: 1.4054 | g_loss: 0.6755\n",
      "Epoch [   38/  200] | d_loss: 1.3978 | g_loss: 0.6763\n",
      "Epoch [   38/  200] | d_loss: 1.4075 | g_loss: 0.6751\n",
      "Epoch [   39/  200] | d_loss: 1.4037 | g_loss: 0.6762\n",
      "Epoch [   39/  200] | d_loss: 1.4028 | g_loss: 0.6770\n",
      "Epoch [   39/  200] | d_loss: 1.4053 | g_loss: 0.6758\n",
      "Epoch [   40/  200] | d_loss: 1.3987 | g_loss: 0.6765\n",
      "Epoch [   40/  200] | d_loss: 1.4014 | g_loss: 0.6767\n",
      "Epoch [   40/  200] | d_loss: 1.3997 | g_loss: 0.6751\n",
      "Epoch [   41/  200] | d_loss: 1.4070 | g_loss: 0.6753\n",
      "Epoch [   41/  200] | d_loss: 1.4091 | g_loss: 0.6765\n",
      "Epoch [   41/  200] | d_loss: 1.4047 | g_loss: 0.6759\n",
      "Epoch [   42/  200] | d_loss: 1.3959 | g_loss: 0.6750\n",
      "Epoch [   42/  200] | d_loss: 1.4027 | g_loss: 0.6754\n",
      "Epoch [   42/  200] | d_loss: 1.3984 | g_loss: 0.6764\n",
      "Epoch [   43/  200] | d_loss: 1.4037 | g_loss: 0.6748\n",
      "Epoch [   43/  200] | d_loss: 1.3992 | g_loss: 0.6751\n",
      "Epoch [   43/  200] | d_loss: 1.4013 | g_loss: 0.6765\n",
      "Epoch [   44/  200] | d_loss: 1.4012 | g_loss: 0.6753\n",
      "Epoch [   44/  200] | d_loss: 1.4028 | g_loss: 0.6759\n",
      "Epoch [   44/  200] | d_loss: 1.4047 | g_loss: 0.6755\n",
      "Epoch [   45/  200] | d_loss: 1.4019 | g_loss: 0.6769\n",
      "Epoch [   45/  200] | d_loss: 1.4011 | g_loss: 0.6762\n",
      "Epoch [   45/  200] | d_loss: 1.4058 | g_loss: 0.6777\n",
      "Epoch [   46/  200] | d_loss: 1.4015 | g_loss: 0.6759\n",
      "Epoch [   46/  200] | d_loss: 1.4013 | g_loss: 0.6754\n",
      "Epoch [   46/  200] | d_loss: 1.4041 | g_loss: 0.6767\n",
      "Epoch [   47/  200] | d_loss: 1.4008 | g_loss: 0.6770\n",
      "Epoch [   47/  200] | d_loss: 1.4102 | g_loss: 0.6769\n",
      "Epoch [   47/  200] | d_loss: 1.4027 | g_loss: 0.6760\n",
      "Epoch [   48/  200] | d_loss: 1.4021 | g_loss: 0.6761\n",
      "Epoch [   48/  200] | d_loss: 1.4017 | g_loss: 0.6759\n",
      "Epoch [   48/  200] | d_loss: 1.4013 | g_loss: 0.6766\n",
      "Epoch [   49/  200] | d_loss: 1.4007 | g_loss: 0.6758\n",
      "Epoch [   49/  200] | d_loss: 1.4046 | g_loss: 0.6766\n",
      "Epoch [   49/  200] | d_loss: 1.3994 | g_loss: 0.6762\n",
      "Epoch [   50/  200] | d_loss: 1.3985 | g_loss: 0.6759\n",
      "Epoch [   50/  200] | d_loss: 1.4054 | g_loss: 0.6758\n",
      "Epoch [   50/  200] | d_loss: 1.4041 | g_loss: 0.6757\n",
      "Epoch [   51/  200] | d_loss: 1.3997 | g_loss: 0.6752\n",
      "Epoch [   51/  200] | d_loss: 1.4048 | g_loss: 0.6764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [   51/  200] | d_loss: 1.4016 | g_loss: 0.6755\n",
      "Epoch [   52/  200] | d_loss: 1.4020 | g_loss: 0.6772\n",
      "Epoch [   52/  200] | d_loss: 1.4030 | g_loss: 0.6759\n",
      "Epoch [   52/  200] | d_loss: 1.4017 | g_loss: 0.6759\n",
      "Epoch [   53/  200] | d_loss: 1.3969 | g_loss: 0.6764\n",
      "Epoch [   53/  200] | d_loss: 1.4026 | g_loss: 0.6744\n",
      "Epoch [   53/  200] | d_loss: 1.3965 | g_loss: 0.6740\n",
      "Epoch [   54/  200] | d_loss: 1.4073 | g_loss: 0.6763\n",
      "Epoch [   54/  200] | d_loss: 1.4021 | g_loss: 0.6753\n",
      "Epoch [   54/  200] | d_loss: 1.4024 | g_loss: 0.6768\n",
      "Epoch [   55/  200] | d_loss: 1.4005 | g_loss: 0.6769\n",
      "Epoch [   55/  200] | d_loss: 1.4056 | g_loss: 0.6732\n",
      "Epoch [   55/  200] | d_loss: 1.3985 | g_loss: 0.6769\n",
      "Epoch [   56/  200] | d_loss: 1.4074 | g_loss: 0.6753\n",
      "Epoch [   56/  200] | d_loss: 1.4047 | g_loss: 0.6766\n",
      "Epoch [   56/  200] | d_loss: 1.4016 | g_loss: 0.6761\n",
      "Epoch [   57/  200] | d_loss: 1.3918 | g_loss: 0.6746\n",
      "Epoch [   57/  200] | d_loss: 1.4019 | g_loss: 0.6763\n",
      "Epoch [   57/  200] | d_loss: 1.3988 | g_loss: 0.6757\n",
      "Epoch [   58/  200] | d_loss: 1.4023 | g_loss: 0.6751\n",
      "Epoch [   58/  200] | d_loss: 1.4008 | g_loss: 0.6746\n",
      "Epoch [   58/  200] | d_loss: 1.4035 | g_loss: 0.6758\n",
      "Epoch [   59/  200] | d_loss: 1.4037 | g_loss: 0.6751\n",
      "Epoch [   59/  200] | d_loss: 1.4053 | g_loss: 0.6756\n",
      "Epoch [   59/  200] | d_loss: 1.4026 | g_loss: 0.6746\n",
      "Epoch [   60/  200] | d_loss: 1.4009 | g_loss: 0.6735\n",
      "Epoch [   60/  200] | d_loss: 1.4015 | g_loss: 0.6763\n",
      "Epoch [   60/  200] | d_loss: 1.4035 | g_loss: 0.6758\n",
      "Epoch [   61/  200] | d_loss: 1.3998 | g_loss: 0.6765\n",
      "Epoch [   61/  200] | d_loss: 1.3990 | g_loss: 0.6746\n",
      "Epoch [   61/  200] | d_loss: 1.4014 | g_loss: 0.6758\n",
      "Epoch [   62/  200] | d_loss: 1.4004 | g_loss: 0.6753\n",
      "Epoch [   62/  200] | d_loss: 1.4066 | g_loss: 0.6764\n",
      "Epoch [   62/  200] | d_loss: 1.3987 | g_loss: 0.6769\n",
      "Epoch [   63/  200] | d_loss: 1.3997 | g_loss: 0.6757\n",
      "Epoch [   63/  200] | d_loss: 1.4011 | g_loss: 0.6759\n",
      "Epoch [   63/  200] | d_loss: 1.4044 | g_loss: 0.6760\n",
      "Epoch [   64/  200] | d_loss: 1.4013 | g_loss: 0.6767\n",
      "Epoch [   64/  200] | d_loss: 1.4026 | g_loss: 0.6755\n",
      "Epoch [   64/  200] | d_loss: 1.4001 | g_loss: 0.6768\n",
      "Epoch [   65/  200] | d_loss: 1.3997 | g_loss: 0.6742\n",
      "Epoch [   65/  200] | d_loss: 1.3982 | g_loss: 0.6757\n",
      "Epoch [   65/  200] | d_loss: 1.3983 | g_loss: 0.6747\n",
      "Epoch [   66/  200] | d_loss: 1.4005 | g_loss: 0.6754\n",
      "Epoch [   66/  200] | d_loss: 1.3978 | g_loss: 0.6751\n",
      "Epoch [   66/  200] | d_loss: 1.3996 | g_loss: 0.6768\n",
      "Epoch [   67/  200] | d_loss: 1.4011 | g_loss: 0.6760\n",
      "Epoch [   67/  200] | d_loss: 1.3983 | g_loss: 0.6754\n",
      "Epoch [   67/  200] | d_loss: 1.4010 | g_loss: 0.6763\n",
      "Epoch [   68/  200] | d_loss: 1.4008 | g_loss: 0.6775\n",
      "Epoch [   68/  200] | d_loss: 1.3924 | g_loss: 0.6759\n",
      "Epoch [   68/  200] | d_loss: 1.4051 | g_loss: 0.6759\n",
      "Epoch [   69/  200] | d_loss: 1.4005 | g_loss: 0.6740\n",
      "Epoch [   69/  200] | d_loss: 1.4072 | g_loss: 0.6751\n",
      "Epoch [   69/  200] | d_loss: 1.3999 | g_loss: 0.6765\n",
      "Epoch [   70/  200] | d_loss: 1.4022 | g_loss: 0.6752\n",
      "Epoch [   70/  200] | d_loss: 1.4031 | g_loss: 0.6759\n",
      "Epoch [   70/  200] | d_loss: 1.4031 | g_loss: 0.6756\n",
      "Epoch [   71/  200] | d_loss: 1.4019 | g_loss: 0.6756\n",
      "Epoch [   71/  200] | d_loss: 1.4021 | g_loss: 0.6764\n",
      "Epoch [   71/  200] | d_loss: 1.4006 | g_loss: 0.6758\n",
      "Epoch [   72/  200] | d_loss: 1.4078 | g_loss: 0.6760\n",
      "Epoch [   72/  200] | d_loss: 1.4049 | g_loss: 0.6747\n",
      "Epoch [   72/  200] | d_loss: 1.3993 | g_loss: 0.6762\n",
      "Epoch [   73/  200] | d_loss: 1.3983 | g_loss: 0.6764\n",
      "Epoch [   73/  200] | d_loss: 1.4043 | g_loss: 0.6751\n",
      "Epoch [   73/  200] | d_loss: 1.4014 | g_loss: 0.6742\n",
      "Epoch [   74/  200] | d_loss: 1.3963 | g_loss: 0.6751\n",
      "Epoch [   74/  200] | d_loss: 1.4060 | g_loss: 0.6777\n",
      "Epoch [   74/  200] | d_loss: 1.4015 | g_loss: 0.6767\n",
      "Epoch [   75/  200] | d_loss: 1.3995 | g_loss: 0.6752\n",
      "Epoch [   75/  200] | d_loss: 1.4073 | g_loss: 0.6745\n",
      "Epoch [   75/  200] | d_loss: 1.3991 | g_loss: 0.6754\n",
      "Epoch [   76/  200] | d_loss: 1.4007 | g_loss: 0.6753\n",
      "Epoch [   76/  200] | d_loss: 1.3991 | g_loss: 0.6750\n",
      "Epoch [   76/  200] | d_loss: 1.4004 | g_loss: 0.6756\n",
      "Epoch [   77/  200] | d_loss: 1.4041 | g_loss: 0.6749\n",
      "Epoch [   77/  200] | d_loss: 1.4038 | g_loss: 0.6749\n",
      "Epoch [   77/  200] | d_loss: 1.3969 | g_loss: 0.6770\n",
      "Epoch [   78/  200] | d_loss: 1.3995 | g_loss: 0.6765\n",
      "Epoch [   78/  200] | d_loss: 1.4048 | g_loss: 0.6742\n",
      "Epoch [   78/  200] | d_loss: 1.3967 | g_loss: 0.6752\n",
      "Epoch [   79/  200] | d_loss: 1.4027 | g_loss: 0.6759\n",
      "Epoch [   79/  200] | d_loss: 1.4007 | g_loss: 0.6744\n",
      "Epoch [   79/  200] | d_loss: 1.3998 | g_loss: 0.6761\n",
      "Epoch [   80/  200] | d_loss: 1.4047 | g_loss: 0.6754\n",
      "Epoch [   80/  200] | d_loss: 1.4026 | g_loss: 0.6764\n",
      "Epoch [   80/  200] | d_loss: 1.4034 | g_loss: 0.6757\n",
      "Epoch [   81/  200] | d_loss: 1.3967 | g_loss: 0.6763\n",
      "Epoch [   81/  200] | d_loss: 1.3961 | g_loss: 0.6758\n",
      "Epoch [   81/  200] | d_loss: 1.4014 | g_loss: 0.6765\n",
      "Epoch [   82/  200] | d_loss: 1.3968 | g_loss: 0.6758\n",
      "Epoch [   82/  200] | d_loss: 1.4034 | g_loss: 0.6760\n",
      "Epoch [   82/  200] | d_loss: 1.3969 | g_loss: 0.6751\n",
      "Epoch [   83/  200] | d_loss: 1.3982 | g_loss: 0.6751\n",
      "Epoch [   83/  200] | d_loss: 1.4013 | g_loss: 0.6765\n",
      "Epoch [   83/  200] | d_loss: 1.4020 | g_loss: 0.6756\n",
      "Epoch [   84/  200] | d_loss: 1.3931 | g_loss: 0.6752\n",
      "Epoch [   84/  200] | d_loss: 1.3998 | g_loss: 0.6775\n",
      "Epoch [   84/  200] | d_loss: 1.3995 | g_loss: 0.6782\n",
      "Epoch [   85/  200] | d_loss: 1.3980 | g_loss: 0.6747\n",
      "Epoch [   85/  200] | d_loss: 1.4062 | g_loss: 0.6744\n",
      "Epoch [   85/  200] | d_loss: 1.4003 | g_loss: 0.6764\n",
      "Epoch [   86/  200] | d_loss: 1.4016 | g_loss: 0.6747\n",
      "Epoch [   86/  200] | d_loss: 1.4006 | g_loss: 0.6756\n",
      "Epoch [   86/  200] | d_loss: 1.4025 | g_loss: 0.6755\n",
      "Epoch [   87/  200] | d_loss: 1.3995 | g_loss: 0.6756\n",
      "Epoch [   87/  200] | d_loss: 1.4063 | g_loss: 0.6762\n",
      "Epoch [   87/  200] | d_loss: 1.4097 | g_loss: 0.6753\n",
      "Epoch [   88/  200] | d_loss: 1.3967 | g_loss: 0.6750\n",
      "Epoch [   88/  200] | d_loss: 1.4032 | g_loss: 0.6745\n",
      "Epoch [   88/  200] | d_loss: 1.4051 | g_loss: 0.6761\n",
      "Epoch [   89/  200] | d_loss: 1.3973 | g_loss: 0.6764\n",
      "Epoch [   89/  200] | d_loss: 1.4097 | g_loss: 0.6756\n",
      "Epoch [   89/  200] | d_loss: 1.4042 | g_loss: 0.6764\n",
      "Epoch [   90/  200] | d_loss: 1.4044 | g_loss: 0.6750\n",
      "Epoch [   90/  200] | d_loss: 1.4095 | g_loss: 0.6753\n",
      "Epoch [   90/  200] | d_loss: 1.4057 | g_loss: 0.6742\n",
      "Epoch [   91/  200] | d_loss: 1.3984 | g_loss: 0.6745\n",
      "Epoch [   91/  200] | d_loss: 1.4053 | g_loss: 0.6755\n",
      "Epoch [   91/  200] | d_loss: 1.4039 | g_loss: 0.6758\n",
      "Epoch [   92/  200] | d_loss: 1.3983 | g_loss: 0.6756\n",
      "Epoch [   92/  200] | d_loss: 1.4035 | g_loss: 0.6763\n",
      "Epoch [   92/  200] | d_loss: 1.4047 | g_loss: 0.6768\n",
      "Epoch [   93/  200] | d_loss: 1.3982 | g_loss: 0.6770\n",
      "Epoch [   93/  200] | d_loss: 1.4038 | g_loss: 0.6753\n",
      "Epoch [   93/  200] | d_loss: 1.4078 | g_loss: 0.6775\n",
      "Epoch [   94/  200] | d_loss: 1.4052 | g_loss: 0.6759\n",
      "Epoch [   94/  200] | d_loss: 1.4014 | g_loss: 0.6759\n",
      "Epoch [   94/  200] | d_loss: 1.3975 | g_loss: 0.6759\n",
      "Epoch [   95/  200] | d_loss: 1.3972 | g_loss: 0.6754\n",
      "Epoch [   95/  200] | d_loss: 1.4017 | g_loss: 0.6755\n",
      "Epoch [   95/  200] | d_loss: 1.3992 | g_loss: 0.6756\n",
      "Epoch [   96/  200] | d_loss: 1.4063 | g_loss: 0.6763\n",
      "Epoch [   96/  200] | d_loss: 1.4022 | g_loss: 0.6747\n",
      "Epoch [   96/  200] | d_loss: 1.3989 | g_loss: 0.6758\n",
      "Epoch [   97/  200] | d_loss: 1.4047 | g_loss: 0.6754\n",
      "Epoch [   97/  200] | d_loss: 1.4080 | g_loss: 0.6749\n",
      "Epoch [   97/  200] | d_loss: 1.3988 | g_loss: 0.6758\n",
      "Epoch [   98/  200] | d_loss: 1.4012 | g_loss: 0.6757\n",
      "Epoch [   98/  200] | d_loss: 1.4035 | g_loss: 0.6764\n",
      "Epoch [   98/  200] | d_loss: 1.3968 | g_loss: 0.6758\n",
      "Epoch [   99/  200] | d_loss: 1.4011 | g_loss: 0.6762\n",
      "Epoch [   99/  200] | d_loss: 1.4061 | g_loss: 0.6759\n",
      "Epoch [   99/  200] | d_loss: 1.4023 | g_loss: 0.6764\n",
      "Epoch [  100/  200] | d_loss: 1.4018 | g_loss: 0.6755\n",
      "Epoch [  100/  200] | d_loss: 1.4078 | g_loss: 0.6764\n",
      "Epoch [  100/  200] | d_loss: 1.3943 | g_loss: 0.6777\n",
      "Epoch [  101/  200] | d_loss: 1.3948 | g_loss: 0.6769\n",
      "Epoch [  101/  200] | d_loss: 1.4085 | g_loss: 0.6748\n",
      "Epoch [  101/  200] | d_loss: 1.4015 | g_loss: 0.6762\n",
      "Epoch [  102/  200] | d_loss: 1.3971 | g_loss: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  102/  200] | d_loss: 1.4027 | g_loss: 0.6753\n",
      "Epoch [  102/  200] | d_loss: 1.4027 | g_loss: 0.6772\n",
      "Epoch [  103/  200] | d_loss: 1.4017 | g_loss: 0.6752\n",
      "Epoch [  103/  200] | d_loss: 1.3983 | g_loss: 0.6746\n",
      "Epoch [  103/  200] | d_loss: 1.3998 | g_loss: 0.6756\n",
      "Epoch [  104/  200] | d_loss: 1.3987 | g_loss: 0.6760\n",
      "Epoch [  104/  200] | d_loss: 1.4081 | g_loss: 0.6757\n",
      "Epoch [  104/  200] | d_loss: 1.4027 | g_loss: 0.6750\n",
      "Epoch [  105/  200] | d_loss: 1.3979 | g_loss: 0.6763\n",
      "Epoch [  105/  200] | d_loss: 1.4064 | g_loss: 0.6746\n",
      "Epoch [  105/  200] | d_loss: 1.3978 | g_loss: 0.6747\n",
      "Epoch [  106/  200] | d_loss: 1.4061 | g_loss: 0.6747\n",
      "Epoch [  106/  200] | d_loss: 1.4075 | g_loss: 0.6753\n",
      "Epoch [  106/  200] | d_loss: 1.4044 | g_loss: 0.6743\n",
      "Epoch [  107/  200] | d_loss: 1.3972 | g_loss: 0.6763\n",
      "Epoch [  107/  200] | d_loss: 1.3990 | g_loss: 0.6756\n",
      "Epoch [  107/  200] | d_loss: 1.3978 | g_loss: 0.6761\n",
      "Epoch [  108/  200] | d_loss: 1.4005 | g_loss: 0.6758\n",
      "Epoch [  108/  200] | d_loss: 1.4043 | g_loss: 0.6749\n",
      "Epoch [  108/  200] | d_loss: 1.4036 | g_loss: 0.6756\n",
      "Epoch [  109/  200] | d_loss: 1.4028 | g_loss: 0.6774\n",
      "Epoch [  109/  200] | d_loss: 1.4055 | g_loss: 0.6762\n",
      "Epoch [  109/  200] | d_loss: 1.4021 | g_loss: 0.6745\n",
      "Epoch [  110/  200] | d_loss: 1.3999 | g_loss: 0.6766\n",
      "Epoch [  110/  200] | d_loss: 1.4022 | g_loss: 0.6754\n",
      "Epoch [  110/  200] | d_loss: 1.4045 | g_loss: 0.6751\n",
      "Epoch [  111/  200] | d_loss: 1.4027 | g_loss: 0.6750\n",
      "Epoch [  111/  200] | d_loss: 1.4043 | g_loss: 0.6761\n",
      "Epoch [  111/  200] | d_loss: 1.4036 | g_loss: 0.6751\n",
      "Epoch [  112/  200] | d_loss: 1.4031 | g_loss: 0.6764\n",
      "Epoch [  112/  200] | d_loss: 1.4038 | g_loss: 0.6744\n",
      "Epoch [  112/  200] | d_loss: 1.4055 | g_loss: 0.6753\n",
      "Epoch [  113/  200] | d_loss: 1.4019 | g_loss: 0.6772\n",
      "Epoch [  113/  200] | d_loss: 1.3998 | g_loss: 0.6762\n",
      "Epoch [  113/  200] | d_loss: 1.4011 | g_loss: 0.6764\n",
      "Epoch [  114/  200] | d_loss: 1.4084 | g_loss: 0.6768\n",
      "Epoch [  114/  200] | d_loss: 1.4040 | g_loss: 0.6743\n",
      "Epoch [  114/  200] | d_loss: 1.3974 | g_loss: 0.6747\n",
      "Epoch [  115/  200] | d_loss: 1.4012 | g_loss: 0.6759\n",
      "Epoch [  115/  200] | d_loss: 1.4043 | g_loss: 0.6744\n",
      "Epoch [  115/  200] | d_loss: 1.4061 | g_loss: 0.6761\n",
      "Epoch [  116/  200] | d_loss: 1.3990 | g_loss: 0.6748\n",
      "Epoch [  116/  200] | d_loss: 1.4030 | g_loss: 0.6771\n",
      "Epoch [  116/  200] | d_loss: 1.3990 | g_loss: 0.6749\n",
      "Epoch [  117/  200] | d_loss: 1.3945 | g_loss: 0.6756\n",
      "Epoch [  117/  200] | d_loss: 1.4087 | g_loss: 0.6750\n",
      "Epoch [  117/  200] | d_loss: 1.3995 | g_loss: 0.6763\n",
      "Epoch [  118/  200] | d_loss: 1.4032 | g_loss: 0.6760\n",
      "Epoch [  118/  200] | d_loss: 1.4013 | g_loss: 0.6751\n",
      "Epoch [  118/  200] | d_loss: 1.4002 | g_loss: 0.6763\n",
      "Epoch [  119/  200] | d_loss: 1.3980 | g_loss: 0.6761\n",
      "Epoch [  119/  200] | d_loss: 1.4072 | g_loss: 0.6760\n",
      "Epoch [  119/  200] | d_loss: 1.3917 | g_loss: 0.6765\n",
      "Epoch [  120/  200] | d_loss: 1.3981 | g_loss: 0.6755\n",
      "Epoch [  120/  200] | d_loss: 1.4079 | g_loss: 0.6739\n",
      "Epoch [  120/  200] | d_loss: 1.3973 | g_loss: 0.6754\n",
      "Epoch [  121/  200] | d_loss: 1.4008 | g_loss: 0.6750\n",
      "Epoch [  121/  200] | d_loss: 1.4043 | g_loss: 0.6764\n",
      "Epoch [  121/  200] | d_loss: 1.4018 | g_loss: 0.6760\n",
      "Epoch [  122/  200] | d_loss: 1.3978 | g_loss: 0.6740\n",
      "Epoch [  122/  200] | d_loss: 1.3967 | g_loss: 0.6767\n",
      "Epoch [  122/  200] | d_loss: 1.4019 | g_loss: 0.6772\n",
      "Epoch [  123/  200] | d_loss: 1.3990 | g_loss: 0.6771\n",
      "Epoch [  123/  200] | d_loss: 1.3989 | g_loss: 0.6772\n",
      "Epoch [  123/  200] | d_loss: 1.4028 | g_loss: 0.6778\n",
      "Epoch [  124/  200] | d_loss: 1.3995 | g_loss: 0.6750\n",
      "Epoch [  124/  200] | d_loss: 1.4041 | g_loss: 0.6757\n",
      "Epoch [  124/  200] | d_loss: 1.4015 | g_loss: 0.6763\n",
      "Epoch [  125/  200] | d_loss: 1.4020 | g_loss: 0.6751\n",
      "Epoch [  125/  200] | d_loss: 1.4048 | g_loss: 0.6743\n",
      "Epoch [  125/  200] | d_loss: 1.4004 | g_loss: 0.6765\n",
      "Epoch [  126/  200] | d_loss: 1.4067 | g_loss: 0.6765\n",
      "Epoch [  126/  200] | d_loss: 1.4037 | g_loss: 0.6750\n",
      "Epoch [  126/  200] | d_loss: 1.3936 | g_loss: 0.6757\n",
      "Epoch [  127/  200] | d_loss: 1.4027 | g_loss: 0.6751\n",
      "Epoch [  127/  200] | d_loss: 1.4059 | g_loss: 0.6763\n",
      "Epoch [  127/  200] | d_loss: 1.4007 | g_loss: 0.6756\n",
      "Epoch [  128/  200] | d_loss: 1.4055 | g_loss: 0.6763\n",
      "Epoch [  128/  200] | d_loss: 1.3973 | g_loss: 0.6754\n",
      "Epoch [  128/  200] | d_loss: 1.3948 | g_loss: 0.6757\n",
      "Epoch [  129/  200] | d_loss: 1.4099 | g_loss: 0.6759\n",
      "Epoch [  129/  200] | d_loss: 1.4057 | g_loss: 0.6768\n",
      "Epoch [  129/  200] | d_loss: 1.3983 | g_loss: 0.6761\n",
      "Epoch [  130/  200] | d_loss: 1.3989 | g_loss: 0.6763\n",
      "Epoch [  130/  200] | d_loss: 1.4004 | g_loss: 0.6753\n",
      "Epoch [  130/  200] | d_loss: 1.4013 | g_loss: 0.6763\n",
      "Epoch [  131/  200] | d_loss: 1.4022 | g_loss: 0.6752\n",
      "Epoch [  131/  200] | d_loss: 1.4005 | g_loss: 0.6757\n",
      "Epoch [  131/  200] | d_loss: 1.4034 | g_loss: 0.6763\n",
      "Epoch [  132/  200] | d_loss: 1.4018 | g_loss: 0.6758\n",
      "Epoch [  132/  200] | d_loss: 1.4020 | g_loss: 0.6749\n",
      "Epoch [  132/  200] | d_loss: 1.4044 | g_loss: 0.6750\n",
      "Epoch [  133/  200] | d_loss: 1.3962 | g_loss: 0.6778\n",
      "Epoch [  133/  200] | d_loss: 1.4062 | g_loss: 0.6761\n",
      "Epoch [  133/  200] | d_loss: 1.4057 | g_loss: 0.6761\n",
      "Epoch [  134/  200] | d_loss: 1.4014 | g_loss: 0.6752\n",
      "Epoch [  134/  200] | d_loss: 1.4031 | g_loss: 0.6757\n",
      "Epoch [  134/  200] | d_loss: 1.3997 | g_loss: 0.6739\n",
      "Epoch [  135/  200] | d_loss: 1.4048 | g_loss: 0.6755\n",
      "Epoch [  135/  200] | d_loss: 1.3990 | g_loss: 0.6766\n",
      "Epoch [  135/  200] | d_loss: 1.4000 | g_loss: 0.6761\n",
      "Epoch [  136/  200] | d_loss: 1.3992 | g_loss: 0.6772\n",
      "Epoch [  136/  200] | d_loss: 1.3941 | g_loss: 0.6750\n",
      "Epoch [  136/  200] | d_loss: 1.4007 | g_loss: 0.6749\n",
      "Epoch [  137/  200] | d_loss: 1.3998 | g_loss: 0.6742\n",
      "Epoch [  137/  200] | d_loss: 1.4046 | g_loss: 0.6751\n",
      "Epoch [  137/  200] | d_loss: 1.3994 | g_loss: 0.6769\n",
      "Epoch [  138/  200] | d_loss: 1.4044 | g_loss: 0.6755\n",
      "Epoch [  138/  200] | d_loss: 1.4022 | g_loss: 0.6757\n",
      "Epoch [  138/  200] | d_loss: 1.3956 | g_loss: 0.6754\n",
      "Epoch [  139/  200] | d_loss: 1.4025 | g_loss: 0.6752\n",
      "Epoch [  139/  200] | d_loss: 1.4030 | g_loss: 0.6752\n",
      "Epoch [  139/  200] | d_loss: 1.4007 | g_loss: 0.6744\n",
      "Epoch [  140/  200] | d_loss: 1.3992 | g_loss: 0.6758\n",
      "Epoch [  140/  200] | d_loss: 1.4088 | g_loss: 0.6743\n",
      "Epoch [  140/  200] | d_loss: 1.3972 | g_loss: 0.6760\n",
      "Epoch [  141/  200] | d_loss: 1.4028 | g_loss: 0.6764\n",
      "Epoch [  141/  200] | d_loss: 1.4004 | g_loss: 0.6750\n",
      "Epoch [  141/  200] | d_loss: 1.3972 | g_loss: 0.6768\n",
      "Epoch [  142/  200] | d_loss: 1.3964 | g_loss: 0.6764\n",
      "Epoch [  142/  200] | d_loss: 1.4060 | g_loss: 0.6756\n",
      "Epoch [  142/  200] | d_loss: 1.3984 | g_loss: 0.6767\n",
      "Epoch [  143/  200] | d_loss: 1.4006 | g_loss: 0.6761\n",
      "Epoch [  143/  200] | d_loss: 1.4025 | g_loss: 0.6751\n",
      "Epoch [  143/  200] | d_loss: 1.3994 | g_loss: 0.6758\n",
      "Epoch [  144/  200] | d_loss: 1.3924 | g_loss: 0.6758\n",
      "Epoch [  144/  200] | d_loss: 1.3962 | g_loss: 0.6752\n",
      "Epoch [  144/  200] | d_loss: 1.4043 | g_loss: 0.6754\n",
      "Epoch [  145/  200] | d_loss: 1.3998 | g_loss: 0.6746\n",
      "Epoch [  145/  200] | d_loss: 1.4039 | g_loss: 0.6754\n",
      "Epoch [  145/  200] | d_loss: 1.4045 | g_loss: 0.6749\n",
      "Epoch [  146/  200] | d_loss: 1.4065 | g_loss: 0.6752\n",
      "Epoch [  146/  200] | d_loss: 1.4085 | g_loss: 0.6762\n",
      "Epoch [  146/  200] | d_loss: 1.3991 | g_loss: 0.6759\n",
      "Epoch [  147/  200] | d_loss: 1.3995 | g_loss: 0.6756\n",
      "Epoch [  147/  200] | d_loss: 1.4013 | g_loss: 0.6743\n",
      "Epoch [  147/  200] | d_loss: 1.3996 | g_loss: 0.6752\n",
      "Epoch [  148/  200] | d_loss: 1.3916 | g_loss: 0.6748\n",
      "Epoch [  148/  200] | d_loss: 1.4032 | g_loss: 0.6754\n",
      "Epoch [  148/  200] | d_loss: 1.4037 | g_loss: 0.6757\n",
      "Epoch [  149/  200] | d_loss: 1.4026 | g_loss: 0.6768\n",
      "Epoch [  149/  200] | d_loss: 1.4022 | g_loss: 0.6756\n",
      "Epoch [  149/  200] | d_loss: 1.4041 | g_loss: 0.6757\n",
      "Epoch [  150/  200] | d_loss: 1.4004 | g_loss: 0.6757\n",
      "Epoch [  150/  200] | d_loss: 1.3978 | g_loss: 0.6748\n",
      "Epoch [  150/  200] | d_loss: 1.4012 | g_loss: 0.6753\n",
      "Epoch [  151/  200] | d_loss: 1.4057 | g_loss: 0.6759\n",
      "Epoch [  151/  200] | d_loss: 1.4027 | g_loss: 0.6756\n",
      "Epoch [  151/  200] | d_loss: 1.3989 | g_loss: 0.6762\n",
      "Epoch [  152/  200] | d_loss: 1.4019 | g_loss: 0.6765\n",
      "Epoch [  152/  200] | d_loss: 1.4004 | g_loss: 0.6745\n",
      "Epoch [  152/  200] | d_loss: 1.3994 | g_loss: 0.6745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  153/  200] | d_loss: 1.4016 | g_loss: 0.6758\n",
      "Epoch [  153/  200] | d_loss: 1.4030 | g_loss: 0.6757\n",
      "Epoch [  153/  200] | d_loss: 1.4046 | g_loss: 0.6753\n",
      "Epoch [  154/  200] | d_loss: 1.3944 | g_loss: 0.6755\n",
      "Epoch [  154/  200] | d_loss: 1.4035 | g_loss: 0.6767\n",
      "Epoch [  154/  200] | d_loss: 1.4003 | g_loss: 0.6756\n",
      "Epoch [  155/  200] | d_loss: 1.4008 | g_loss: 0.6744\n",
      "Epoch [  155/  200] | d_loss: 1.4076 | g_loss: 0.6745\n",
      "Epoch [  155/  200] | d_loss: 1.3995 | g_loss: 0.6755\n",
      "Epoch [  156/  200] | d_loss: 1.4025 | g_loss: 0.6763\n",
      "Epoch [  156/  200] | d_loss: 1.4040 | g_loss: 0.6743\n",
      "Epoch [  156/  200] | d_loss: 1.3997 | g_loss: 0.6760\n",
      "Epoch [  157/  200] | d_loss: 1.4054 | g_loss: 0.6754\n",
      "Epoch [  157/  200] | d_loss: 1.4050 | g_loss: 0.6740\n",
      "Epoch [  157/  200] | d_loss: 1.3909 | g_loss: 0.6755\n",
      "Epoch [  158/  200] | d_loss: 1.3955 | g_loss: 0.6762\n",
      "Epoch [  158/  200] | d_loss: 1.4086 | g_loss: 0.6751\n",
      "Epoch [  158/  200] | d_loss: 1.3976 | g_loss: 0.6754\n",
      "Epoch [  159/  200] | d_loss: 1.3974 | g_loss: 0.6760\n",
      "Epoch [  159/  200] | d_loss: 1.4016 | g_loss: 0.6755\n",
      "Epoch [  159/  200] | d_loss: 1.4020 | g_loss: 0.6753\n",
      "Epoch [  160/  200] | d_loss: 1.4070 | g_loss: 0.6756\n",
      "Epoch [  160/  200] | d_loss: 1.4067 | g_loss: 0.6760\n",
      "Epoch [  160/  200] | d_loss: 1.4019 | g_loss: 0.6777\n",
      "Epoch [  161/  200] | d_loss: 1.4009 | g_loss: 0.6753\n",
      "Epoch [  161/  200] | d_loss: 1.4049 | g_loss: 0.6750\n",
      "Epoch [  161/  200] | d_loss: 1.3947 | g_loss: 0.6765\n",
      "Epoch [  162/  200] | d_loss: 1.3994 | g_loss: 0.6754\n",
      "Epoch [  162/  200] | d_loss: 1.4015 | g_loss: 0.6748\n",
      "Epoch [  162/  200] | d_loss: 1.3988 | g_loss: 0.6748\n",
      "Epoch [  163/  200] | d_loss: 1.4077 | g_loss: 0.6744\n",
      "Epoch [  163/  200] | d_loss: 1.4002 | g_loss: 0.6768\n",
      "Epoch [  163/  200] | d_loss: 1.4011 | g_loss: 0.6746\n",
      "Epoch [  164/  200] | d_loss: 1.3997 | g_loss: 0.6774\n",
      "Epoch [  164/  200] | d_loss: 1.3982 | g_loss: 0.6764\n",
      "Epoch [  164/  200] | d_loss: 1.4040 | g_loss: 0.6754\n",
      "Epoch [  165/  200] | d_loss: 1.3956 | g_loss: 0.6750\n",
      "Epoch [  165/  200] | d_loss: 1.4022 | g_loss: 0.6764\n",
      "Epoch [  165/  200] | d_loss: 1.3968 | g_loss: 0.6764\n",
      "Epoch [  166/  200] | d_loss: 1.3990 | g_loss: 0.6757\n",
      "Epoch [  166/  200] | d_loss: 1.4055 | g_loss: 0.6754\n",
      "Epoch [  166/  200] | d_loss: 1.4000 | g_loss: 0.6765\n",
      "Epoch [  167/  200] | d_loss: 1.4053 | g_loss: 0.6764\n",
      "Epoch [  167/  200] | d_loss: 1.3991 | g_loss: 0.6762\n",
      "Epoch [  167/  200] | d_loss: 1.4039 | g_loss: 0.6756\n",
      "Epoch [  168/  200] | d_loss: 1.4034 | g_loss: 0.6750\n",
      "Epoch [  168/  200] | d_loss: 1.4064 | g_loss: 0.6748\n",
      "Epoch [  168/  200] | d_loss: 1.3995 | g_loss: 0.6764\n",
      "Epoch [  169/  200] | d_loss: 1.4038 | g_loss: 0.6748\n",
      "Epoch [  169/  200] | d_loss: 1.4060 | g_loss: 0.6747\n",
      "Epoch [  169/  200] | d_loss: 1.3991 | g_loss: 0.6758\n",
      "Epoch [  170/  200] | d_loss: 1.4018 | g_loss: 0.6750\n",
      "Epoch [  170/  200] | d_loss: 1.3995 | g_loss: 0.6748\n",
      "Epoch [  170/  200] | d_loss: 1.4030 | g_loss: 0.6751\n",
      "Epoch [  171/  200] | d_loss: 1.3954 | g_loss: 0.6743\n",
      "Epoch [  171/  200] | d_loss: 1.4047 | g_loss: 0.6748\n",
      "Epoch [  171/  200] | d_loss: 1.3987 | g_loss: 0.6759\n",
      "Epoch [  172/  200] | d_loss: 1.4002 | g_loss: 0.6756\n",
      "Epoch [  172/  200] | d_loss: 1.4076 | g_loss: 0.6725\n",
      "Epoch [  172/  200] | d_loss: 1.3997 | g_loss: 0.6773\n",
      "Epoch [  173/  200] | d_loss: 1.3990 | g_loss: 0.6749\n",
      "Epoch [  173/  200] | d_loss: 1.4043 | g_loss: 0.6750\n",
      "Epoch [  173/  200] | d_loss: 1.4067 | g_loss: 0.6736\n",
      "Epoch [  174/  200] | d_loss: 1.3981 | g_loss: 0.6765\n",
      "Epoch [  174/  200] | d_loss: 1.4046 | g_loss: 0.6749\n",
      "Epoch [  174/  200] | d_loss: 1.3958 | g_loss: 0.6754\n",
      "Epoch [  175/  200] | d_loss: 1.3993 | g_loss: 0.6752\n",
      "Epoch [  175/  200] | d_loss: 1.4031 | g_loss: 0.6762\n",
      "Epoch [  175/  200] | d_loss: 1.4045 | g_loss: 0.6761\n",
      "Epoch [  176/  200] | d_loss: 1.4015 | g_loss: 0.6760\n",
      "Epoch [  176/  200] | d_loss: 1.4042 | g_loss: 0.6752\n",
      "Epoch [  176/  200] | d_loss: 1.4024 | g_loss: 0.6751\n",
      "Epoch [  177/  200] | d_loss: 1.3956 | g_loss: 0.6760\n",
      "Epoch [  177/  200] | d_loss: 1.4042 | g_loss: 0.6748\n",
      "Epoch [  177/  200] | d_loss: 1.4086 | g_loss: 0.6753\n",
      "Epoch [  178/  200] | d_loss: 1.4003 | g_loss: 0.6760\n",
      "Epoch [  178/  200] | d_loss: 1.4028 | g_loss: 0.6743\n",
      "Epoch [  178/  200] | d_loss: 1.4037 | g_loss: 0.6756\n",
      "Epoch [  179/  200] | d_loss: 1.4061 | g_loss: 0.6757\n",
      "Epoch [  179/  200] | d_loss: 1.3999 | g_loss: 0.6745\n",
      "Epoch [  179/  200] | d_loss: 1.4003 | g_loss: 0.6762\n",
      "Epoch [  180/  200] | d_loss: 1.3963 | g_loss: 0.6754\n",
      "Epoch [  180/  200] | d_loss: 1.3997 | g_loss: 0.6755\n",
      "Epoch [  180/  200] | d_loss: 1.3981 | g_loss: 0.6755\n",
      "Epoch [  181/  200] | d_loss: 1.4037 | g_loss: 0.6767\n",
      "Epoch [  181/  200] | d_loss: 1.4014 | g_loss: 0.6761\n",
      "Epoch [  181/  200] | d_loss: 1.3998 | g_loss: 0.6746\n",
      "Epoch [  182/  200] | d_loss: 1.3982 | g_loss: 0.6767\n",
      "Epoch [  182/  200] | d_loss: 1.4043 | g_loss: 0.6759\n",
      "Epoch [  182/  200] | d_loss: 1.3971 | g_loss: 0.6753\n",
      "Epoch [  183/  200] | d_loss: 1.4000 | g_loss: 0.6756\n",
      "Epoch [  183/  200] | d_loss: 1.4087 | g_loss: 0.6760\n",
      "Epoch [  183/  200] | d_loss: 1.4001 | g_loss: 0.6756\n",
      "Epoch [  184/  200] | d_loss: 1.3974 | g_loss: 0.6751\n",
      "Epoch [  184/  200] | d_loss: 1.3985 | g_loss: 0.6763\n",
      "Epoch [  184/  200] | d_loss: 1.4091 | g_loss: 0.6755\n",
      "Epoch [  185/  200] | d_loss: 1.3983 | g_loss: 0.6750\n",
      "Epoch [  185/  200] | d_loss: 1.4006 | g_loss: 0.6753\n",
      "Epoch [  185/  200] | d_loss: 1.4040 | g_loss: 0.6751\n",
      "Epoch [  186/  200] | d_loss: 1.3992 | g_loss: 0.6754\n",
      "Epoch [  186/  200] | d_loss: 1.4006 | g_loss: 0.6764\n",
      "Epoch [  186/  200] | d_loss: 1.3999 | g_loss: 0.6743\n",
      "Epoch [  187/  200] | d_loss: 1.3973 | g_loss: 0.6753\n",
      "Epoch [  187/  200] | d_loss: 1.4068 | g_loss: 0.6757\n",
      "Epoch [  187/  200] | d_loss: 1.4055 | g_loss: 0.6751\n",
      "Epoch [  188/  200] | d_loss: 1.4062 | g_loss: 0.6768\n",
      "Epoch [  188/  200] | d_loss: 1.4013 | g_loss: 0.6762\n",
      "Epoch [  188/  200] | d_loss: 1.3976 | g_loss: 0.6759\n",
      "Epoch [  189/  200] | d_loss: 1.3950 | g_loss: 0.6758\n",
      "Epoch [  189/  200] | d_loss: 1.4039 | g_loss: 0.6757\n",
      "Epoch [  189/  200] | d_loss: 1.3990 | g_loss: 0.6747\n",
      "Epoch [  190/  200] | d_loss: 1.4018 | g_loss: 0.6759\n",
      "Epoch [  190/  200] | d_loss: 1.4075 | g_loss: 0.6770\n",
      "Epoch [  190/  200] | d_loss: 1.4042 | g_loss: 0.6768\n",
      "Epoch [  191/  200] | d_loss: 1.4052 | g_loss: 0.6754\n",
      "Epoch [  191/  200] | d_loss: 1.4035 | g_loss: 0.6750\n",
      "Epoch [  191/  200] | d_loss: 1.4007 | g_loss: 0.6743\n",
      "Epoch [  192/  200] | d_loss: 1.4008 | g_loss: 0.6746\n",
      "Epoch [  192/  200] | d_loss: 1.4047 | g_loss: 0.6733\n",
      "Epoch [  192/  200] | d_loss: 1.3991 | g_loss: 0.6772\n",
      "Epoch [  193/  200] | d_loss: 1.4038 | g_loss: 0.6758\n",
      "Epoch [  193/  200] | d_loss: 1.4005 | g_loss: 0.6752\n",
      "Epoch [  193/  200] | d_loss: 1.4035 | g_loss: 0.6763\n",
      "Epoch [  194/  200] | d_loss: 1.3999 | g_loss: 0.6754\n",
      "Epoch [  194/  200] | d_loss: 1.3988 | g_loss: 0.6771\n",
      "Epoch [  194/  200] | d_loss: 1.4040 | g_loss: 0.6748\n",
      "Epoch [  195/  200] | d_loss: 1.4009 | g_loss: 0.6754\n",
      "Epoch [  195/  200] | d_loss: 1.4056 | g_loss: 0.6783\n",
      "Epoch [  195/  200] | d_loss: 1.4014 | g_loss: 0.6755\n",
      "Epoch [  196/  200] | d_loss: 1.4090 | g_loss: 0.6755\n",
      "Epoch [  196/  200] | d_loss: 1.4080 | g_loss: 0.6747\n",
      "Epoch [  196/  200] | d_loss: 1.4024 | g_loss: 0.6757\n",
      "Epoch [  197/  200] | d_loss: 1.3978 | g_loss: 0.6758\n",
      "Epoch [  197/  200] | d_loss: 1.3969 | g_loss: 0.6760\n",
      "Epoch [  197/  200] | d_loss: 1.4044 | g_loss: 0.6776\n",
      "Epoch [  198/  200] | d_loss: 1.3980 | g_loss: 0.6749\n",
      "Epoch [  198/  200] | d_loss: 1.4003 | g_loss: 0.6760\n",
      "Epoch [  198/  200] | d_loss: 1.4028 | g_loss: 0.6766\n",
      "Epoch [  199/  200] | d_loss: 1.3986 | g_loss: 0.6758\n",
      "Epoch [  199/  200] | d_loss: 1.4010 | g_loss: 0.6754\n",
      "Epoch [  199/  200] | d_loss: 1.3978 | g_loss: 0.6754\n",
      "Epoch [  200/  200] | d_loss: 1.3950 | g_loss: 0.6740\n",
      "Epoch [  200/  200] | d_loss: 1.3996 | g_loss: 0.6763\n",
      "Epoch [  200/  200] | d_loss: 1.4072 | g_loss: 0.6763\n",
      "elapsed_time = 61.0 min, 3719.9399189949036 sec\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# training hyperparams\n",
    "num_epochs2 = 200\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples2 = []\n",
    "losses2 = []\n",
    "\n",
    "print_every2 = 400\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance\n",
    "sample_size=16\n",
    "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size2))\n",
    "fixed_z = torch.from_numpy(fixed_z).float()\n",
    "\n",
    "# train the network\n",
    "D2.train()\n",
    "G2.train()\n",
    "for epoch in range(num_epochs2):\n",
    "    \n",
    "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
    "                \n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        ## Important rescaling step ## \n",
    "        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n",
    "        \n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATOR\n",
    "        # ============================================\n",
    "        \n",
    "        d_optimizer2.zero_grad()\n",
    "        \n",
    "        # 1. Train with real images\n",
    "\n",
    "        # Compute the discriminator losses on real images \n",
    "        # smooth the real labels\n",
    "        D_real2 = D2(real_images)\n",
    "        d_real_loss2 = real_loss(D_real2, smooth=True)\n",
    "        \n",
    "        # 2. Train with fake images\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size2))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        fake_images = G2(z)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images        \n",
    "        D_fake2 = D2(fake_images)\n",
    "        d_fake_loss2 = fake_loss(D_fake2)\n",
    "        \n",
    "        # add up loss and perform backprop\n",
    "        d_loss2 = d_real_loss2 + d_fake_loss2\n",
    "        d_loss2.backward()\n",
    "        d_optimizer2.step()\n",
    "        \n",
    "        \n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATOR\n",
    "        # =========================================\n",
    "        g_optimizer2.zero_grad()\n",
    "        \n",
    "        # 1. Train with fake images and flipped labels\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size2))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        fake_images = G2(z)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images \n",
    "        # using flipped labels!\n",
    "        D_fake2 = D2(fake_images)\n",
    "        g_loss2 = real_loss(D_fake2) # use real loss to flip labels\n",
    "        \n",
    "        # perform backprop\n",
    "        g_loss2.backward()\n",
    "        g_optimizer2.step()\n",
    "\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every2 == 0:\n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs2, d_loss2.item(), g_loss2.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##\n",
    "    # append discriminator loss and generator loss\n",
    "    losses2.append((d_loss2.item(), g_loss2.item()))\n",
    "    \n",
    "    # generate and save sample, fake images\n",
    "    G2.eval() # eval mode for generating samples\n",
    "    samples_z = G2(fixed_z)\n",
    "    samples2.append(samples_z)\n",
    "    G2.train() # back to train mode\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"elapsed_time = {} min, {} sec\".format(elapsed_time//60, elapsed_time))\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples2.pkl', 'wb') as f:\n",
    "    pkl.dump(samples2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
